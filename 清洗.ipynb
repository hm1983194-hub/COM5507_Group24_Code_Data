{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615e7785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰å·¥ä½œç›®å½•: C:\\Users\\86155\\Desktop\n",
      "æ›´æ”¹åå·¥ä½œç›®å½•: D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\n",
      "æ–°ç›®å½•ä¸‹çš„æ–‡ä»¶:\n",
      "  ğŸ“„ carousell_dimoo.csv\n",
      "  ğŸ“„ carousell_labubu.csv\n",
      "  ğŸ“„ carousell_molly.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# æŸ¥çœ‹å½“å‰å·¥ä½œç›®å½•\n",
    "print(\"å½“å‰å·¥ä½œç›®å½•:\", os.getcwd())\n",
    "\n",
    "# æ›´æ”¹å·¥ä½œç›®å½•åˆ°æŒ‡å®šè·¯å¾„\n",
    "new_path = r\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\"  # æ›¿æ¢ä¸ºä½ æƒ³è¦çš„è·¯å¾„\n",
    "os.chdir(new_path)\n",
    "\n",
    "# éªŒè¯æ˜¯å¦æ›´æ”¹æˆåŠŸ\n",
    "print(\"æ›´æ”¹åå·¥ä½œç›®å½•:\", os.getcwd())\n",
    "\n",
    "# æŸ¥çœ‹æ–°ç›®å½•ä¸‹çš„æ–‡ä»¶\n",
    "print(\"æ–°ç›®å½•ä¸‹çš„æ–‡ä»¶:\")\n",
    "for file in os.listdir('.'):\n",
    "    if file.endswith('.csv'):\n",
    "        print(f\"  ğŸ“„ {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df37f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ•°æ®æ¡æ•°: 10534\n",
      "æ¸…æ´—åæ•°æ®æ¡æ•°: 10216\n",
      "è¿‡æ»¤æ‰äº† 318 æ¡æ•°æ®\n",
      "âœ… æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: cleaned_molly_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# æ–¹æ³•2ï¼šä½¿ç”¨æ­£æ–œæ ï¼ˆWindowsä¹Ÿæ”¯æŒï¼‰\n",
    "df = pd.read_csv(\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\carousell_molly.csv\")\n",
    "print(f\"åŸå§‹æ•°æ®æ¡æ•°: {len(df)}\")\n",
    "\n",
    "# å…¶ä½™ä»£ç ç›¸åŒ...\n",
    "def contains_molly_related(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    \n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    if 'molly' not in text_lower:\n",
    "        return False\n",
    "    \n",
    "    cleaned_text = re.sub(r'[^\\w]', ' ', text_lower).strip()\n",
    "    words = [word for word in cleaned_text.split() if len(word) > 2]\n",
    "    \n",
    "    if len(words) == 1 and words[0] == 'molly':\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "df_cleaned = df[df['å•†å“åç§°'].apply(contains_molly_related)]\n",
    "\n",
    "print(f\"æ¸…æ´—åæ•°æ®æ¡æ•°: {len(df_cleaned)}\")\n",
    "print(f\"è¿‡æ»¤æ‰äº† {len(df) - len(df_cleaned)} æ¡æ•°æ®\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "df_cleaned.to_csv(\"cleaned_molly_cleaned.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: cleaned_molly_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bda7d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ•°æ®æ¡æ•°: 10536\n",
      "æ¸…æ´—åæ•°æ®æ¡æ•°: 9976\n",
      "è¿‡æ»¤æ‰äº† 560 æ¡æ•°æ®\n",
      "âœ… æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: cleaned_dimoo_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# æ–¹æ³•2ï¼šä½¿ç”¨æ­£æ–œæ ï¼ˆWindowsä¹Ÿæ”¯æŒï¼‰\n",
    "df = pd.read_csv(\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\carousell_dimoo.csv\")\n",
    "print(f\"åŸå§‹æ•°æ®æ¡æ•°: {len(df)}\")\n",
    "\n",
    "# å…¶ä½™ä»£ç ç›¸åŒ...\n",
    "def contains_dimoo_related(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    \n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    if 'dimoo' not in text_lower:\n",
    "        return False\n",
    "    \n",
    "    cleaned_text = re.sub(r'[^\\w]', ' ', text_lower).strip()\n",
    "    words = [word for word in cleaned_text.split() if len(word) > 2]\n",
    "    \n",
    "    if len(words) == 1 and words[0] == 'dimoo':\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "df_cleaned = df[df['å•†å“åç§°'].apply(contains_dimoo_related)]\n",
    "\n",
    "print(f\"æ¸…æ´—åæ•°æ®æ¡æ•°: {len(df_cleaned)}\")\n",
    "print(f\"è¿‡æ»¤æ‰äº† {len(df) - len(df_cleaned)} æ¡æ•°æ®\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "df_cleaned.to_csv(\"cleaned_dimoo_cleaned.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: cleaned_dimoo_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# æ–¹æ³•2ï¼šä½¿ç”¨æ­£æ–œæ ï¼ˆWindowsä¹Ÿæ”¯æŒï¼‰\n",
    "df = pd.read_csv(\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\carousell_labubu.csv\")\n",
    "print(f\"åŸå§‹æ•°æ®æ¡æ•°: {len(df)}\")\n",
    "\n",
    "# å…¶ä½™ä»£ç ç›¸åŒ...\n",
    "def contains_labubu_related(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    \n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    if 'labubu' not in text_lower:\n",
    "        return False\n",
    "    \n",
    "    cleaned_text = re.sub(r'[^\\w]', ' ', text_lower).strip()\n",
    "    words = [word for word in cleaned_text.split() if len(word) > 2]\n",
    "    \n",
    "    if len(words) == 1 and words[0] == 'labubu':\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "df_cleaned = df[df['å•†å“åç§°'].apply(contains_labubu_related)]\n",
    "\n",
    "print(f\"æ¸…æ´—åæ•°æ®æ¡æ•°: {len(df_cleaned)}\")\n",
    "print(f\"è¿‡æ»¤æ‰äº† {len(df) - len(df_cleaned)} æ¡æ•°æ®\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "df_cleaned.to_csv(\"cleaned_labubu_cleaned.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: cleaned_labubu_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c32e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ•°æ®æ¡æ•°: 10528\n",
      "æ¸…æ´—åæ•°æ®æ¡æ•°: 10228\n",
      "è¿‡æ»¤æ‰äº† 300 æ¡æ•°æ®\n",
      "âœ… æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: cleaned_skullpanda_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\carousell_skullpanda.csv\")\n",
    "print(f\"åŸå§‹æ•°æ®æ¡æ•°: {len(df)}\")\n",
    "\n",
    "def contains_skullpanda_related(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    \n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    if 'skullpanda' not in text_lower:\n",
    "        return False\n",
    "    \n",
    "    cleaned_text = re.sub(r'[^\\w]', ' ', text_lower).strip()\n",
    "    words = [word for word in cleaned_text.split() if len(word) > 2]\n",
    "    \n",
    "    if len(words) == 1 and words[0] == 'skullpanda':\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "df_cleaned = df[df['å•†å“åç§°'].apply(contains_skullpanda_related)]\n",
    "\n",
    "print(f\"æ¸…æ´—åæ•°æ®æ¡æ•°: {len(df_cleaned)}\")\n",
    "print(f\"è¿‡æ»¤æ‰äº† {len(df) - len(df_cleaned)} æ¡æ•°æ®\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "df_cleaned.to_csv(\"cleaned_skullpanda_cleaned.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: cleaned_skullpanda_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb4c75d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ•°æ®æ¡æ•°: 1048\n",
      "=== è°ƒè¯•æµ‹è¯• ===\n",
      "\n",
      "DEBUG å¤„ç†: Sweet Bean å¤©ä½¿ï¼ˆå·²é–‹åŒ…è£ï¼‰\n",
      "  - åŒ…å«sweetbean: True\n",
      "  - åŒ…å«å°ç”œè±†: False\n",
      "  - ç§»é™¤å…³é”®è¯å: ' å¤©ä½¿ï¼ˆå·²é–‹åŒ…è£ï¼‰'\n",
      "  - æœ‰æ„ä¹‰è¯æ±‡: ['å¤©ä½¿', 'å·²é–‹åŒ…è£']\n",
      "  -> ä¿ç•™\n",
      "æœ€ç»ˆç»“æœ: True\n",
      "\n",
      "DEBUG å¤„ç†: Sweet Bean è–èª•ç¯€éš±è—ç‰ˆå…¬ä»”\n",
      "  - åŒ…å«sweetbean: True\n",
      "  - åŒ…å«å°ç”œè±†: False\n",
      "  - ç§»é™¤å…³é”®è¯å: ' è–èª•ç¯€éš±è—ç‰ˆå…¬ä»”'\n",
      "  - æœ‰æ„ä¹‰è¯æ±‡: ['è–èª•ç¯€éš±è—ç‰ˆå…¬ä»”']\n",
      "  -> ä¿ç•™\n",
      "æœ€ç»ˆç»“æœ: True\n",
      "\n",
      "DEBUG å¤„ç†: Sweet Beanå±¬æ–¼ä½ çš„ç¯€æ—¥ç³»åˆ—-é€å‡ºç¦®ç‰©\n",
      "  - åŒ…å«sweetbean: True\n",
      "  - åŒ…å«å°ç”œè±†: False\n",
      "  - ç§»é™¤å…³é”®è¯å: 'å±¬æ–¼ä½ çš„ç¯€æ—¥ç³»åˆ—-é€å‡ºç¦®ç‰©'\n",
      "  - æœ‰æ„ä¹‰è¯æ±‡: ['å±¬æ–¼ä½ çš„ç¯€æ—¥ç³»åˆ—', 'é€å‡ºç¦®ç‰©']\n",
      "  -> ä¿ç•™\n",
      "æœ€ç»ˆç»“æœ: True\n",
      "\n",
      "DEBUG å¤„ç†: Pop Mart Sweet Bean Angel's Blessing\n",
      "  - åŒ…å«sweetbean: True\n",
      "  - åŒ…å«å°ç”œè±†: False\n",
      "  - ç§»é™¤å…³é”®è¯å: 'Pop Mart  Angel's Blessing'\n",
      "  - æœ‰æ„ä¹‰è¯æ±‡: ['Pop', 'Mart', 'Angel', 'Blessing']\n",
      "  -> ä¿ç•™\n",
      "æœ€ç»ˆç»“æœ: True\n",
      "\n",
      "DEBUG å¤„ç†: POP MART Sweet Bean Frozen Time Dessert Box\n",
      "  - åŒ…å«sweetbean: True\n",
      "  - åŒ…å«å°ç”œè±†: False\n",
      "  - ç§»é™¤å…³é”®è¯å: 'POP MART  Frozen Time Dessert Box'\n",
      "  - æœ‰æ„ä¹‰è¯æ±‡: ['POP', 'MART', 'Frozen', 'Time', 'Dessert', 'Box']\n",
      "  -> ä¿ç•™\n",
      "æœ€ç»ˆç»“æœ: True\n",
      "\n",
      "DEBUG å¤„ç†: sweetbean\n",
      "  - åŒ…å«sweetbean: True\n",
      "  - åŒ…å«å°ç”œè±†: False\n",
      "  - ç§»é™¤å…³é”®è¯å: ''\n",
      "  - æœ‰æ„ä¹‰è¯æ±‡: []\n",
      "  -> æ²¡æœ‰å…¶ä»–å†…å®¹ï¼Œè¿‡æ»¤\n",
      "æœ€ç»ˆç»“æœ: False\n",
      "\n",
      "DEBUG å¤„ç†: å°ç”œè±†\n",
      "  - åŒ…å«sweetbean: False\n",
      "  - åŒ…å«å°ç”œè±†: True\n",
      "  - ç§»é™¤å…³é”®è¯å: ''\n",
      "  - æœ‰æ„ä¹‰è¯æ±‡: []\n",
      "  -> æ²¡æœ‰å…¶ä»–å†…å®¹ï¼Œè¿‡æ»¤\n",
      "æœ€ç»ˆç»“æœ: False\n",
      "\n",
      "DEBUG å¤„ç†: sweet bean 123\n",
      "  - åŒ…å«sweetbean: True\n",
      "  - åŒ…å«å°ç”œè±†: False\n",
      "  - ç§»é™¤å…³é”®è¯å: ' 123'\n",
      "  - æœ‰æ„ä¹‰è¯æ±‡: ['123']\n",
      "  -> ä¿ç•™\n",
      "æœ€ç»ˆç»“æœ: True\n",
      "\n",
      "==================================================\n",
      "å¼€å§‹æ­£å¼è¿‡æ»¤...\n",
      "æ¸…æ´—åæ•°æ®æ¡æ•°: 1020\n",
      "è¿‡æ»¤æ‰äº† 28 æ¡æ•°æ®\n",
      "\n",
      "è¢«è¿‡æ»¤æ‰çš„å•†å“ç¤ºä¾‹:\n",
      "ç¤ºä¾‹ 1: å°ç”œè±† Sweet Bean\n",
      "ç¤ºä¾‹ 2: å°ç”œè±† Sweet Bean\n",
      "ç¤ºä¾‹ 3: Popmart ç›²ç›’ crybaby labubu å°é‡ skullpanda Azura finding Mokoko æ˜Ÿæ˜Ÿäºº å¾—å¾—b TinyTiny PinoJelly Pucky ç”œè±† HOTTOYS Cosbi æ¸…å±‹åƒ¹ åªé™ä¸Šæ°´äº¤æ”¶\n",
      "ç¤ºä¾‹ 4: ğŸ‚ Pop Mart ç›²ç›’ Celebrating The Moment æ…¶ç¥é€™ä¸€åˆ» Pop Mart 15é€±å¹´ 15å‘¨å¹´ å€’æ•¸æ™‚åˆ» / Mollyæ°£æ°›æ“”ç•¶ / SPå¸·å¹•ä¹‹ä¸‹ / Crybabyé»äº®æ­¤åˆ» / Dimooç«¥å¿ƒæœªæ³¯ / å°é‡321ç™¼å°„ / æ˜Ÿæ˜Ÿäººå¹èµ·è™Ÿè§’ / Hacipupuè¨±é¡˜ç¦®ç‰© / Zsigaèµ´ç´„ / Puckyé–ƒé–ƒç™»å ´ / ç”œè±†åˆ†äº«ç”œèœœ / Pino Jellyæ˜Ÿå…‰ä¹‹ç´„ / Chakaä»™å¥³æ•™æ¯ / Kuboæ™‚å…‰çš„è¿´éŸ¿ / Polarç›¡æƒ…åœ°å¹å§å˜Ÿå˜Ÿ / éš±è— æ´¾å°ç”œå¿ƒ ğŸ‚ é è³¼\n",
      "ç¤ºä¾‹ 5: Sweet Bean\n",
      "ç¤ºä¾‹ 6: Sweet Bean\n",
      "ç¤ºä¾‹ 7: Sweet bean\n",
      "ç¤ºä¾‹ 8: æ—¥æœ¬æ‰­è›‹ é¼æ³°è±Š è¿·ä½ æ›é£¾ å¯æ€æ¬¾ æ¨¡å‹ æ“ºè¨­ æ‰­è›‹ ç©å…· capsule toy æ—¥æœ¬ä»£è³¼ æ—¥æœ¬ç›´é€ å°ç± åŒ… ç”œè±† èŸ¹ç²‰ è¦ä»ç‚’é£¯ ç‰›è‚‰éºµ å€‹åˆ¥æ¬¾å¼æœƒå› æ‡‰ç•¶æ™‚å¸‚å ´ä¸Šåƒ¹æ ¼è€Œèª¿æ•´\n",
      "ç¤ºä¾‹ 9: sweet bean å°ç”œè±†\n",
      "ç¤ºä¾‹ 10: sweet bean\n",
      "\n",
      "ä¿ç•™çš„å•†å“ç¤ºä¾‹:\n",
      "ç¤ºä¾‹ 1: Popmart å°ç”œè±† sweet bean ï¼»æ›ï¼½\n",
      "ç¤ºä¾‹ 2: popmartå°ç”œè±†è–èª• å¤©ä½¿ç¥ç¦ æ‰‹è¾¦\n",
      "ç¤ºä¾‹ 3: POPMART èšå…‰ç‡ˆä¸‹13å‘¨å¹´æ›ä»¶ç›²ç›’ç³»åˆ— Molly å¤§éƒ½æœƒ $50 LiLiOS-é›¨èˆ‡èˆ $30 å°ç”œè±†sweet bean-åˆå¾Œ Cozy afternoon $30 ğŸ’°$110 å…¨åŒ…4å€‹ğŸ’° å…¨æ–°æœªæ‹†è¢‹ åªæ‹†ç›’ç¢ºèªè§’è‰²âœ… POPMART æ³¡æ³¡ç‘ªç‰¹\n",
      "ç¤ºä¾‹ 4: å°ç”œè±† sweet beanæ„›çš„æŠ±æŠ±ç³»åˆ—æ‰‹è¾¦ç›²ç›’-ç•¶æˆ‘ç”Ÿæ°£æ™‚ POPMART æ³¡æ³¡ç‘ªç‰¹ å…¨æ–° åªæ‹†ç›’ç¢ºèªè§’è‰²æœªæ‹†è¢‹âœ…\n",
      "ç¤ºä¾‹ 5: POPMART å°ç”œè±† å±¬æ–¼ä½ çš„ç¯€æ—¥ç³»åˆ—æ‰‹è¾¦â„ï¸éš±è—æ¬¾æ°´æ™¶çƒ\n",
      "ç¤ºä¾‹ 6: PopMart sweet bean å°ç”œè±†å±¬æ–¼ä½ çš„ç¯€æ—¥ç³»åˆ—æ‰‹è¾¦ è–‘é¤…äººåŠé£¾\n",
      "ç¤ºä¾‹ 7: Sweet Bean è–èª• å°ç”œè±† æ”¶åˆ°å¿ƒé¡˜\n",
      "ç¤ºä¾‹ 8: Popmart Popbean æ…¶ç¥é€™ä¸€åˆ»èŒç²’ SPæ›ä¸Šæ’ä¸æ›puckyï¼Œå°ç”œè±†æ›pucky/zsiga/kubo/polarï¼Œ2æ›2å„ªå…ˆ\n",
      "ç¤ºä¾‹ 9: Popmart æ³¡æ³¡ç‘ªç‰¹å°ç”œè±†æ³¡æ³¡ç³»åˆ—\n",
      "ç¤ºä¾‹ 10: Sweet Bean å°ç”œè±† å±¬æ–¼ä½ çš„ç¯€æ—¥ç³»åˆ— é›ªèŠ±\n",
      "\n",
      "âœ… æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: cleaned_sweetbean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\carousell_sweetbean.csv\")\n",
    "print(f\"åŸå§‹æ•°æ®æ¡æ•°: {len(df)}\")\n",
    "\n",
    "def contains_sweetbean_related(text):\n",
    "    \"\"\"\n",
    "    æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«sweetbeanæˆ–å°ç”œè±†ç›¸å…³å†…å®¹\n",
    "    ä»…å‰”é™¤å®Œå…¨åªæœ‰sweetbeanæˆ–å°ç”œè±†çš„æƒ…å†µ\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    \n",
    "    text_str = str(text)\n",
    "    text_lower = text_str.lower()\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦åŒ…å«sweetbeanæˆ–å°ç”œè±†\n",
    "    has_sweetbean = 'sweetbean' in text_lower or 'sweet bean' in text_lower\n",
    "    has_xiaotiandou = 'å°ç”œè±†' in text_str\n",
    "    \n",
    "    # å¦‚æœä¸¤è€…éƒ½ä¸åŒ…å«ï¼Œç›´æ¥è¿”å›False\n",
    "    if not has_sweetbean and not has_xiaotiandou:\n",
    "        return False\n",
    "    \n",
    "    # ç§»é™¤sweetbeanå’Œå°ç”œè±†å…³é”®è¯ï¼Œæ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–å†…å®¹\n",
    "    temp_text = text_str\n",
    "    \n",
    "    # ç§»é™¤sweetbeanç›¸å…³å…³é”®è¯\n",
    "    temp_text = re.sub(r'sweetbean|sweet bean', '', temp_text, flags=re.IGNORECASE)\n",
    "    # ç§»é™¤å°ç”œè±†\n",
    "    temp_text = temp_text.replace('å°ç”œè±†', '')\n",
    "    \n",
    "    # æ¸…ç†æ–‡æœ¬ï¼Œä¿ç•™æœ‰æ„ä¹‰çš„å†…å®¹\n",
    "    cleaned_temp = re.sub(r'[^a-zA-Z0-9\\u4e00-\\u9fff]', ' ', temp_text).strip()\n",
    "    meaningful_words = [word for word in cleaned_temp.split() if len(word) > 1]\n",
    "    \n",
    "    # å¦‚æœç§»é™¤å…³é”®è¯åæ²¡æœ‰å…¶ä»–æœ‰æ„ä¹‰çš„å†…å®¹ï¼Œåˆ™è¿‡æ»¤æ‰\n",
    "    if not meaningful_words:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def debug_contains_sweetbean_related(text):\n",
    "    \"\"\"\n",
    "    è°ƒè¯•ç‰ˆæœ¬ï¼Œæ˜¾ç¤ºå¤„ç†è¿‡ç¨‹\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        print(f\"DEBUG: {text} -> NaN\")\n",
    "        return False\n",
    "    \n",
    "    text_str = str(text)\n",
    "    text_lower = text_str.lower()\n",
    "    \n",
    "    print(f\"\\nDEBUG å¤„ç†: {text_str}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦åŒ…å«sweetbeanæˆ–å°ç”œè±†\n",
    "    has_sweetbean = 'sweetbean' in text_lower or 'sweet bean' in text_lower\n",
    "    has_xiaotiandou = 'å°ç”œè±†' in text_str\n",
    "    \n",
    "    print(f\"  - åŒ…å«sweetbean: {has_sweetbean}\")\n",
    "    print(f\"  - åŒ…å«å°ç”œè±†: {has_xiaotiandou}\")\n",
    "    \n",
    "    # å¦‚æœä¸¤è€…éƒ½ä¸åŒ…å«ï¼Œç›´æ¥è¿”å›False\n",
    "    if not has_sweetbean and not has_xiaotiandou:\n",
    "        print(f\"  -> ä¸åŒ…å«å…³é”®è¯ï¼Œè¿‡æ»¤\")\n",
    "        return False\n",
    "    \n",
    "    # ç§»é™¤sweetbeanå’Œå°ç”œè±†å…³é”®è¯ï¼Œæ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–å†…å®¹\n",
    "    temp_text = text_str\n",
    "    \n",
    "    # ç§»é™¤sweetbeanç›¸å…³å…³é”®è¯\n",
    "    temp_text = re.sub(r'sweetbean|sweet bean', '', temp_text, flags=re.IGNORECASE)\n",
    "    # ç§»é™¤å°ç”œè±†\n",
    "    temp_text = temp_text.replace('å°ç”œè±†', '')\n",
    "    \n",
    "    print(f\"  - ç§»é™¤å…³é”®è¯å: '{temp_text}'\")\n",
    "    \n",
    "    # æ¸…ç†æ–‡æœ¬ï¼Œä¿ç•™æœ‰æ„ä¹‰çš„å†…å®¹\n",
    "    cleaned_temp = re.sub(r'[^a-zA-Z0-9\\u4e00-\\u9fff]', ' ', temp_text).strip()\n",
    "    meaningful_words = [word for word in cleaned_temp.split() if len(word) > 1]\n",
    "    \n",
    "    print(f\"  - æœ‰æ„ä¹‰è¯æ±‡: {meaningful_words}\")\n",
    "    \n",
    "    # å¦‚æœç§»é™¤å…³é”®è¯åæ²¡æœ‰å…¶ä»–æœ‰æ„ä¹‰çš„å†…å®¹ï¼Œåˆ™è¿‡æ»¤æ‰\n",
    "    if not meaningful_words:\n",
    "        print(f\"  -> æ²¡æœ‰å…¶ä»–å†…å®¹ï¼Œè¿‡æ»¤\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"  -> ä¿ç•™\")\n",
    "    return True\n",
    "\n",
    "# å…ˆæµ‹è¯•å‡ ä¸ªè¢«é”™è¯¯è¿‡æ»¤çš„ä¾‹å­\n",
    "test_examples = [\n",
    "    \"Sweet Bean å¤©ä½¿ï¼ˆå·²é–‹åŒ…è£ï¼‰\",\n",
    "    \"Sweet Bean è–èª•ç¯€éš±è—ç‰ˆå…¬ä»”\", \n",
    "    \"Sweet Beanå±¬æ–¼ä½ çš„ç¯€æ—¥ç³»åˆ—-é€å‡ºç¦®ç‰©\",\n",
    "    \"Pop Mart Sweet Bean Angel's Blessing\",\n",
    "    \"POP MART Sweet Bean Frozen Time Dessert Box\",\n",
    "    \"sweetbean\",  # è¿™ä¸ªåº”è¯¥è¢«è¿‡æ»¤\n",
    "    \"å°ç”œè±†\",     # è¿™ä¸ªåº”è¯¥è¢«è¿‡æ»¤\n",
    "    \"sweet bean 123\"  # è¿™ä¸ªåº”è¯¥è¢«è¿‡æ»¤\n",
    "]\n",
    "\n",
    "print(\"=== è°ƒè¯•æµ‹è¯• ===\")\n",
    "for example in test_examples:\n",
    "    result = debug_contains_sweetbean_related(example)\n",
    "    print(f\"æœ€ç»ˆç»“æœ: {result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"å¼€å§‹æ­£å¼è¿‡æ»¤...\")\n",
    "\n",
    "# åº”ç”¨è¿‡æ»¤\n",
    "df_cleaned = df[df['å•†å“åç§°'].apply(contains_sweetbean_related)]\n",
    "\n",
    "print(f\"æ¸…æ´—åæ•°æ®æ¡æ•°: {len(df_cleaned)}\")\n",
    "print(f\"è¿‡æ»¤æ‰äº† {len(df) - len(df_cleaned)} æ¡æ•°æ®\")\n",
    "\n",
    "# æ˜¾ç¤ºè¢«è¿‡æ»¤æ‰çš„ç¤ºä¾‹\n",
    "df_filtered = df[~df.index.isin(df_cleaned.index)]\n",
    "if not df_filtered.empty:\n",
    "    print(f\"\\nè¢«è¿‡æ»¤æ‰çš„å•†å“ç¤ºä¾‹:\")\n",
    "    for i, (idx, row) in enumerate(df_filtered.head(10).iterrows()):\n",
    "        print(f\"ç¤ºä¾‹ {i+1}: {row['å•†å“åç§°']}\")\n",
    "\n",
    "# æ˜¾ç¤ºä¿ç•™çš„å•†å“ç¤ºä¾‹\n",
    "if not df_cleaned.empty:\n",
    "    print(f\"\\nä¿ç•™çš„å•†å“ç¤ºä¾‹:\")\n",
    "    for i, (idx, row) in enumerate(df_cleaned.head(10).iterrows()):\n",
    "        print(f\"ç¤ºä¾‹ {i+1}: {row['å•†å“åç§°']}\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "df_cleaned.to_csv(\"cleaned_sweetbean.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nâœ… æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åˆ°: cleaned_sweetbean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ec82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½äº† 6684 ä¸ªå•†å“æ•°æ®\n",
      "æ­£åœ¨å°†å•†å“åç§°ç”±ç¹ä½“è½¬æ¢ä¸ºç®€ä½“...\n",
      "âœ… å•†å“åç§°ç¹ç®€è½¬æ¢å®Œæˆ\n",
      "\n",
      "=== ç¹ç®€è½¬æ¢ç¤ºä¾‹ ===\n",
      "ç¤ºä¾‹ 1: labubu å‡Œæ™¨ä¸€ç‚¹ ç¾å¤¢æ™‚é–“\n",
      "ç¤ºä¾‹ 2: POP MART Labubu é‡‘è›‡å ±å–œ é‡‘è›‡è³€æ­² æ¯›çµ¨é–€æ›\n",
      "ç¤ºä¾‹ 3: ã€å¥½è©•éç™¾/èª ä¿¡è³£å®¶ã€‘THE MONSTER - å…¨æ–°æœªæ‹†ç›’æ©Ÿæ¬¾/æ‹†ç›’æœªæ‹†è¢‹æ¬¾/ç¤¼ç‰©é¦–é¸/å¿ƒåº•å¯†ç¢¼ç³»åˆ—/mini labubu 4.0/ A-M & N-Z ï¼ˆåƒ¹éŒ¢å„ªæƒ è«‹ç‡è©³æƒ…ï¼‰\n",
      "ç¤ºä¾‹ 4: Pop Mart LABUBU å…¨æ–° è”æè“è“ ç›²ç›’ The Monsters Exciting Macaron Lychee Berry\n",
      "ç¤ºä¾‹ 5: POPMART Labubu åå\n",
      "å¼€å§‹çˆ¬å–å®˜æ–¹å›¾é‰´æ•°æ®...\n",
      "æ­£åœ¨è®¿é—®ç³»åˆ—é¡µé¢...\n",
      "ä½¿ç”¨é€‰æ‹©å™¨ 'a[href*=\"/zh/series/\"]' æ‰¾åˆ° 72 ä¸ªç³»åˆ—\n",
      "å»é‡åå¾—åˆ° 72 ä¸ªç³»åˆ—\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 1/72: [æ–°] Labubu Why So Serious ä¸‡åœ£èŠ‚ç³»åˆ—2025.10\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 23 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 2/72: [æ–°] Labubu å¿ƒåº•ç§˜å¯†ç³»åˆ—2025.08\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 47 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 3/72: Labubu FALL IN WILD Series2024.04\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 7 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 4/72: Labubu å¿«ä¹ä¸‡åœ£èŠ‚æ´¾å¯¹ç³»åˆ—2024.10\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 11 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 5/72: LABUBU Ã— PRONOUNCE - WINGS OF FORTUNE Series2024.10\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 6 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 6/72: Labubu Time to Chill2022.10\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 6 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 7/72: Labubu Angel In Clouds2024.10\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 4 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 8/72: å‰æ–¹é«˜èƒ½ç³»åˆ—-æªèƒ¶æ¯›ç»’æŒ‚ä»¶2025.04\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 14 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 9/72: å¿ƒåŠ¨é©¬å¡é¾™æªèƒ¶è„¸ç³»åˆ—2023.10\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 11 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 10/72: ååæ´¾å¯¹æªèƒ¶æ¯›ç»’ç³»åˆ—2024.07\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 14 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 11/72: æ€ªå‘³ä¾¿åˆ©åº—ç³»åˆ—2025.06\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 25 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 12/72: Labubu Let's Checkmate Series2025.02\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 7 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 13/72: å¯å£å¯ä¹ç³»åˆ—æªèƒ¶2024.12\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 9 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 14/72: Labubu FLIP WITH ME Series2024.07\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 7 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 15/72: THE MONSTERS x èˆªæµ·ç‹ç³»åˆ—2025.02\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 25 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 16/72: æ…µæ‡’ç‘œä¼½ç³»åˆ—2024.08\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 22 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 17/72: å¯»æ‰¾MOKOKOç³»åˆ—2024.09\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 23 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 18/72: æ¨ªå±±å®Ma.K.ç³»åˆ—2023.04\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 25 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 19/72: NAUGHTY PLANTSæªèƒ¶è„¸ç³»åˆ—2023.05\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 12 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 20/72: çœ‹ä¸è§æˆ‘ç³»åˆ—2024.04\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 25 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 21/72: THE MONSTERSå¯å£å¯ä¹ç³»åˆ—æ‰‹åŠ2024.11\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 22 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 22/72: æ°´æœç³»åˆ—2021.12\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 25 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 23/72: ç²¾çµè‰ºæœ¯ç³»åˆ—2020.07\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 21 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 24/72: è¿·ä½ ZIMOMOç³»åˆ—3ä»£2020.08\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 15 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 25/72: ç²¾çµç”œå“ç³»åˆ—2019.12\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 25 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 26/72: åå››å‘¨å¹´æ— åŒé•‡ç³»åˆ—2024.08\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 25 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 27/72: é˜¿æ ¹å»·å›½å®¶é˜Ÿå®˜æ–¹ç³»åˆ—2021.05\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 19 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 28/72: è¿·ä½ ZIMOMOç³»åˆ—4ä»£2021.09\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 14 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 29/72: å¤å¤ç”œèœœç³»åˆ—2022.09\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 25 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 30/72: èŠ±ä¹‹ç²¾çµç³»åˆ—2020.05\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 22 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 31/72: ç²¾çµåŠ¨ç‰©ç³»åˆ—2022.03\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 25 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 32/72: è¿·ä½ ZIMOMOç³»åˆ—1ä»£2018.11\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 15 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 33/72: æ˜Ÿåº§ç³»åˆ—2023.08\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 24 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 34/72: THE MONSTERS xæµ·ç»µå®å®ç³»åˆ—2021.04\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 24 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 35/72: é¡½çš®æ—¥è®°ç³»åˆ—2024.01\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 14 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 36/72: æ£®æ—éŸ³ä¹ä¼šç³»åˆ—2019.04\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 21 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 37/72: ä¸€èµ·åœ£è¯ç³»åˆ—2022.11\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 25 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 38/72: æ€ªç‰©å˜‰å¹´åç³»åˆ—2019.10\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 23 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 39/72: ä¸€èµ·ç©æ¸¸æˆç³»åˆ—2024.09\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 15 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 40/72: é¾™åŸçŒ®ç‘ç³»åˆ—2023.12\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 36 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 41/72: è¿·ä½ ZIMOMOå¥—è£…2020.11\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 15 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 42/72: åœ£è¯ç³»åˆ—2020.12\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 8 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 43/72: ç²¾çµç©å…·ç³»åˆ—2020.12\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 25 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 44/72: ä¹å›­,é›†åˆï¼ç³»åˆ—2023.09\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 12 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 45/72: å®‡å®™å¤§å†’é™©ç³»åˆ—2021.09\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 25 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 46/72: è¿·ä½ ZIMOMOç³»åˆ—2ä»£2019.08\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 14 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 47/72: è¿åŠ¨ç³»åˆ—2019.08\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 13 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 48/72: èšå…‰ç¯ä¸‹åä¸‰å‘¨å¹´ç³»åˆ—2023.10\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 22 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 49/72: é‡‘è›‡è´ºå²Â·æ–°å¹´å¥½ç‰Œç³»åˆ—2024.12\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 18 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 50/72: é‡å¤–æ—…è¡Œç³»åˆ—2020.10\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 13 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 51/72: æ€ªå…½çŒäººç³»åˆ—2.52022.08\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 36 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 52/72: ç²¾çµå¤©å›¢3ä»£Aç‰ˆ2023.10\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 13 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 53/72: POPCARä¹å›­ç¢°ç¢°è½¦ç³»åˆ—2022.12\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 22 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 54/72: ç²¾çµæ£®æ—æ´¾å¯¹ç³»åˆ—2023.09\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 10 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 55/72: å¿ƒæ„¿æŒ‡å°–ç³»åˆ—2023.08\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 18 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 56/72: æµªæ¼«æŒ‡å°–ç³»åˆ—2022.07\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 20 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 57/72: ç²¾çµä¹‹å®¶ç³»åˆ—2023.01\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 20 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 58/72: ç²¾çµå¤©å›¢3ä»£Bç‰ˆ2023.10\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 12 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 59/72: Labubu x Bill Wallåˆä½œåŠé¥°\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 60/72: æµªæ¼«æŒ‡å°–ç³»åˆ—32024.07\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 21 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 61/72: è™è·ƒæ–°æ˜¥ç³»åˆ—2022.01\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 27 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 62/72: MIXè¶…çº§èµ›é“ç³»åˆ—2021.12\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 23 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 63/72: å¯çˆ±ç§å®¶è½¦ç³»åˆ—2022.01\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 23 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 64/72: ç²¾çµæ€ªå…½ç³»åˆ—2022.07\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 22 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 65/72: å²Â·å…”Â·æ—ºç³»åˆ—2022.12\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 29 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 66/72: æ°´ä¸Šæ´¾å¯¹ç³»åˆ—2023.08\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 20 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 67/72: å¿«ä¹å‘¨æœ«ç³»åˆ—2022.10\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 22 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 68/72: ç²¾çµå¤©å›¢2ä»£2019.08\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 11 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 69/72: ç²¾çµå¤©å›¢TTF2017.09\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 10 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 70/72: ç²¾çµå¤©å›¢1ä»£2016.12\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 10 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 71/72: babyç³»åˆ—-RAVIPAæ¬¾2024.11\n",
      "  ä½¿ç”¨é€‰æ‹©å™¨ '.grid > div' æ‰¾åˆ° 10 ä¸ªå•†å“\n",
      "æ­£åœ¨çˆ¬å–ç³»åˆ— 72/72: SUPERGROUP OF THE MONSTERS (MINI FIGURES SET)labubuç²¾çµå¤©å›¢2024.03\n",
      "æ€»å…±çˆ¬å–åˆ° 721 ä¸ªå®˜æ–¹å•†å“\n",
      "å»é‡åå®˜æ–¹å•†å“æ•°é‡: 689\n",
      "âœ… å®˜æ–¹å›¾é‰´æ•°æ®å·²ä¿å­˜åˆ°: D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\catalog_labubu.csv\n",
      "\n",
      "=== å®˜æ–¹å•†å“ç¤ºä¾‹ ===\n",
      "1. æœˆå…‰é¢å…·ï¼ˆè¶…çº§éšè—æ¬¾ï¼‰ - [æ–°] Labubu Why So Serious ä¸‡åœ£èŠ‚ç³»åˆ—2025.10\n",
      "2. æ³¡æ³¡ç³–ï¼ˆéšè—æ¬¾ï¼‰ - [æ–°] Labubu Why So Serious ä¸‡åœ£èŠ‚ç³»åˆ—2025.10\n",
      "3. æ‚è€å°ä¸‘ - [æ–°] Labubu Why So Serious ä¸‡åœ£èŠ‚ç³»åˆ—2025.10\n",
      "4. ç«ç„°å°ä¸‘ - [æ–°] Labubu Why So Serious ä¸‡åœ£èŠ‚ç³»åˆ—2025.10\n",
      "5. é“¶é›¾å°ä¸‘ - [æ–°] Labubu Why So Serious ä¸‡åœ£èŠ‚ç³»åˆ—2025.10\n",
      "6. å…‰æ˜ç››å®´ - [æ–°] Labubu Why So Serious ä¸‡åœ£èŠ‚ç³»åˆ—2025.10\n",
      "7. æçº¿æœ¨å¶ - [æ–°] Labubu Why So Serious ä¸‡åœ£èŠ‚ç³»åˆ—2025.10\n",
      "8. æš—å½±å°ä¸‘ - [æ–°] Labubu Why So Serious ä¸‡åœ£èŠ‚ç³»åˆ—2025.10\n",
      "9. å½©è™¹è½¯ç³– - [æ–°] Labubu Why So Serious ä¸‡åœ£èŠ‚ç³»åˆ—2025.10\n",
      "10. æ·±çº¢èˆè€… - [æ–°] Labubu Why So Serious ä¸‡åœ£èŠ‚ç³»åˆ—2025.10\n",
      "\n",
      "æ­£åœ¨è¿›è¡Œå•†å“åŒ¹é…...\n",
      "æ­£åœ¨é¢„å¤„ç†æ–‡æœ¬...\n",
      "\n",
      "=== é¢„å¤„ç†ç¤ºä¾‹ ===\n",
      "åŸå§‹: labubu å‡Œæ™¨ä¸€ç‚¹ ç¾å¤¢æ™‚é–“ -> æ¸…æ´—: å‡Œæ™¨ä¸€ç‚¹ ç¾å¤¢æ™‚é–“\n",
      "åŸå§‹: POP MART Labubu é‡‘è›‡å ±å–œ é‡‘è›‡è³€æ­² æ¯›çµ¨é–€æ› -> æ¸…æ´—: é‡‘è›‡å ±å–œ é‡‘è›‡è³€æ­² æ¯›çµ¨é–€æ›\n",
      "åŸå§‹: ã€å¥½è©•éç™¾/èª ä¿¡è³£å®¶ã€‘THE MONSTER - å…¨æ–°æœªæ‹†ç›’æ©Ÿæ¬¾/æ‹†ç›’æœªæ‹†è¢‹æ¬¾/ç¤¼ç‰©é¦–é¸/å¿ƒåº•å¯†ç¢¼ç³»åˆ—/mini labubu 4.0/ A-M & N-Z ï¼ˆåƒ¹éŒ¢å„ªæƒ è«‹ç‡è©³æƒ…ï¼‰ -> æ¸…æ´—: å¥½è©•éç™¾èª ä¿¡è³£å®¶the - å…¨æ–°æœªæ‹†ç›’æ©Ÿæ¬¾æ‹†ç›’æœªæ‹†è¢‹æ¬¾ç¤¼ç‰©é¦–é¸å¿ƒåº•å¯†ç¢¼ç³»åˆ—mini 4.0 a-m n-z åƒ¹éŒ¢å„ªæƒ è«‹ç‡è©³æƒ…\n",
      "åŸå§‹: Pop Mart LABUBU å…¨æ–° è”æè“è“ ç›²ç›’ The Monsters Exciting Macaron Lychee Berry -> æ¸…æ´—: è”æè“è“ exciting macaron lychee berry\n",
      "åŸå§‹: POPMART Labubu åå -> æ¸…æ´—: åå\n",
      "\n",
      "æ­£åœ¨è¿›è¡Œå…³é”®è¯è¡¥å……åŒ¹é…...\n",
      "æ„å»ºäº† 785 ä¸ªå…³é”®è¯åˆ°ç³»åˆ—çš„æ˜ å°„\n",
      "é€šè¿‡å…³é”®è¯åŒ¹é…æˆåŠŸåŒ¹é…äº† 5 ä¸ªå•†å“\n",
      "\n",
      "æ­£åœ¨è¿›è¡Œå‡Œæ™¨å…³é”®è¯åå¤„ç†...\n",
      "å‘ç° 130 ä¸ªæœªåŒ¹é…ä½†åŒ…å«å‡Œæ™¨å…³é”®è¯çš„å•†å“ï¼Œå°†å…¶åˆ†ç±»ä¸ºå‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: LABUBU å‡Œæ™¨ä¸€ç‚¹é»ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç›²ç›’ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç›²ç›’ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—æ‰‹è¾¦ é‘‘è³æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨1é»é˜ äº¤æ› -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: ï¼ˆè³£/æ›é€šè©±æ™‚é–“ï¼‰(å…¨æ–°æ‹†ç›’ç¢ºèªæ¬¾ï¼‰popmart labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— çˆ¬è¡Œæ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: ï¼ˆè³£/æ›é€šè©±æ™‚é–“ï¼‰(å…¨æ–°æ‹†ç›’ç¢ºèªæ¬¾ï¼‰popmart labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— çˆ¬è¡Œæ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç›²ç›’ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç›²ç›’ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹é˜ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨1é» æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨1é» æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART THE MONSTERS 01:00 A.M. Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— é–±è®€æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— é–±è®€æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: The monsters 1:00am labubu pop mart -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: The monsters 1:00am labubu pop mart -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: (å…¨æ–°æ‹†ç›’ç¢ºèªæ¬¾ï¼‰popmart labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— é–±è®€æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: (å…¨æ–°æ‹†ç›’ç¢ºèªæ¬¾ï¼‰popmart labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— é–±è®€æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: [å…¨æ–°æœªé–‹å°]Popmart Labubu å‡Œæ™¨ä¸€ç‚¹åŸºæœ¬æ¬¾ä¸€å¥— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: [å…¨æ–°æœªé–‹å°]Popmart Labubu å‡Œæ™¨ä¸€ç‚¹åŸºæœ¬æ¬¾ä¸€å¥— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: å…¨æ–°ç¢ºèªæ¬¾POP MART LABUBU å‡Œæ™¨ä¸€ç‚¹é»ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: å…¨æ–°ç¢ºèªæ¬¾POP MART LABUBU å‡Œæ™¨ä¸€ç‚¹é»ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu The Monster 1am (çˆ¬è¡Œæ™‚é–“ï¼‰ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu The Monster 1am (çˆ¬è¡Œæ™‚é–“ï¼‰ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: The monster å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: The monster å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: labubu é›¶æ™¨ä¸€ç‚¹é˜ ç³»åˆ— ç›²ç›’ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: labubu é›¶æ™¨ä¸€ç‚¹é˜ ç³»åˆ— ç›²ç›’ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Popmart Labubu 1am -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Popmart Labubu 1am -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ é©šæ…„æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ é©šæ…„æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART Labubu å‡Œæ™¨ä¸€ç‚¹ ï¼ˆé©šæ…„æ™‚é–“ï¼‰æ›ï¼ˆé€šè©±æ™‚é–“ï¼‰ ç›²ç›’å…¬ä»” -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART Labubu å‡Œæ™¨ä¸€ç‚¹ ï¼ˆé©šæ…„æ™‚é–“ï¼‰æ›ï¼ˆé€šè©±æ™‚é–“ï¼‰ ç›²ç›’å…¬ä»” -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹é˜ [æ›å…¶ä»–æ¬¾] è—åœˆæ›ç´…åœˆğŸ™ğŸ» -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹é˜ [æ›å…¶ä»–æ¬¾] è—åœˆæ›ç´…åœˆğŸ™ğŸ» -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹é˜ [æ›å…¶ä»–æ¬¾] -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹é˜ [æ›å…¶ä»–æ¬¾] -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: LABUBU å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: LABUBU å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu the monsters å‡Œæ™¨1:00åŸæ¢ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu the monsters å‡Œæ™¨1:00åŸæ¢ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Popmart Labubu å‡Œæ™¨ä¸€ç‚¹æ‰‹è¾¨ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Popmart Labubu å‡Œæ™¨ä¸€ç‚¹æ‰‹è¾¨ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART LABUBU å‡Œæ™¨ä¸€ç‚¹é»ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART LABUBU å‡Œæ™¨ä¸€ç‚¹é»ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Popmart Labubu å‡Œæ™¨ä¸€ç‚¹(æ¢éšªæ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Popmart Labubu å‡Œæ™¨ä¸€ç‚¹(æ¢éšªæ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART Labubu å‡Œæ™¨ä¸€ç‚¹äº¤æ› -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART Labubu å‡Œæ™¨ä¸€ç‚¹äº¤æ› -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: ï¼ˆè³£/æ›é€šè©±æ™‚é–“ï¼‰(å…¨æ–°æ‹†ç›’ç¢ºèªæ¬¾ï¼‰popmart labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— çˆ¬è¡Œæ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: ï¼ˆè³£/æ›é€šè©±æ™‚é–“ï¼‰(å…¨æ–°æ‹†ç›’ç¢ºèªæ¬¾ï¼‰popmart labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— çˆ¬è¡Œæ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨1é» æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨1é» æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— é–±è®€æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— é–±è®€æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: The monsters 1:00am labubu pop mart -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: The monsters 1:00am labubu pop mart -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: (å…¨æ–°æ‹†ç›’ç¢ºèªæ¬¾ï¼‰popmart labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— é–±è®€æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: (å…¨æ–°æ‹†ç›’ç¢ºèªæ¬¾ï¼‰popmart labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— é–±è®€æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: [å…¨æ–°æœªé–‹å°]Popmart Labubu å‡Œæ™¨ä¸€ç‚¹åŸºæœ¬æ¬¾ä¸€å¥— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: [å…¨æ–°æœªé–‹å°]Popmart Labubu å‡Œæ™¨ä¸€ç‚¹åŸºæœ¬æ¬¾ä¸€å¥— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Pop mart The monsters å‡Œæ™¨ä¸€ç‚¹ç³» 1:00am Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: å…¨æ–°ç¢ºèªæ¬¾POP MART LABUBU å‡Œæ™¨ä¸€ç‚¹é»ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: å…¨æ–°ç¢ºèªæ¬¾POP MART LABUBU å‡Œæ™¨ä¸€ç‚¹é»ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu The Monster 1am (çˆ¬è¡Œæ™‚é–“ï¼‰ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu The Monster 1am (çˆ¬è¡Œæ™‚é–“ï¼‰ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: The monster å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: The monster å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— Labubu -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: labubu é›¶æ™¨ä¸€ç‚¹é˜ ç³»åˆ— ç›²ç›’ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: labubu é›¶æ™¨ä¸€ç‚¹é˜ ç³»åˆ— ç›²ç›’ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Popmart Labubu 1am -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Popmart Labubu 1am -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ é©šæ…„æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ é©šæ…„æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART Labubu å‡Œæ™¨ä¸€ç‚¹ ï¼ˆé©šæ…„æ™‚é–“ï¼‰æ›ï¼ˆé€šè©±æ™‚é–“ï¼‰ ç›²ç›’å…¬ä»” -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART Labubu å‡Œæ™¨ä¸€ç‚¹ ï¼ˆé©šæ…„æ™‚é–“ï¼‰æ›ï¼ˆé€šè©±æ™‚é–“ï¼‰ ç›²ç›’å…¬ä»” -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹ç³»åˆ— -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹é˜ [æ›å…¶ä»–æ¬¾] è—åœˆæ›ç´…åœˆğŸ™ğŸ» -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹é˜ [æ›å…¶ä»–æ¬¾] è—åœˆæ›ç´…åœˆğŸ™ğŸ» -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹é˜ [æ›å…¶ä»–æ¬¾] -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu å‡Œæ™¨ä¸€ç‚¹é˜ [æ›å…¶ä»–æ¬¾] -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: LABUBU å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: LABUBU å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu the monsters å‡Œæ™¨1:00åŸæ¢ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu the monsters å‡Œæ™¨1:00åŸæ¢ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Popmart Labubu å‡Œæ™¨ä¸€ç‚¹æ‰‹è¾¨ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Popmart Labubu å‡Œæ™¨ä¸€ç‚¹æ‰‹è¾¨ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART LABUBU å‡Œæ™¨ä¸€ç‚¹é»ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART LABUBU å‡Œæ™¨ä¸€ç‚¹é»ç³»åˆ—ç›²ç›’æ‰‹è¾¦ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Popmart Labubu å‡Œæ™¨ä¸€ç‚¹(æ¢éšªæ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Popmart Labubu å‡Œæ™¨ä¸€ç‚¹(æ¢éšªæ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART Labubu å‡Œæ™¨ä¸€ç‚¹äº¤æ› -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: POP MART Labubu å‡Œæ™¨ä¸€ç‚¹äº¤æ› -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu The Monster 01:00am ç›²ç›’ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu The Monster 01:00am ç›²ç›’ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu The Monster 1:00amç›²ç›’Music Timeé‘‘è³æ™‚é–“ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu The Monster 01:00am ç›²ç›’ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "âœ… åå¤„ç†åŒ¹é…: Labubu The Monster 01:00am ç›²ç›’ -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
      "\n",
      "=== åŒ¹é…ç»“æœç»Ÿè®¡ ===\n",
      "æˆåŠŸåŒ¹é…: 13289 ä»¶å•†å“\n",
      "æœªæˆåŠŸåŒ¹é…: 673 ä»¶å•†å“\n",
      "åŒ¹é…æˆåŠŸç‡: 95.2%\n",
      "\n",
      "=== åŒ¹é…æ–¹å¼ç»Ÿè®¡ ===\n",
      "TF-IDFåŒ¹é…: 13154ä»¶\n",
      "å‡Œæ™¨å…³é”®è¯åå¤„ç†: 130ä»¶\n",
      "å…³é”®è¯åŒ¹é…: 5ä»¶\n",
      "\n",
      "=== æˆåŠŸåŒ¹é…ç¤ºä¾‹ ===\n",
      "å•†å“: labubu å‡Œæ™¨ä¸€ç‚¹ ç¾å¤¢æ™‚é–“\n",
      "å®˜æ–¹: å¹¿åœºç„¦ç‚¹\n",
      "ç³»åˆ—: çœ‹ä¸è§æˆ‘ç³»åˆ—2024.04 (åŒ¹é…åº¦: 0.151, æ–¹å¼: TF-IDFåŒ¹é…)\n",
      "---\n",
      "å•†å“: POP MART Labubu é‡‘è›‡å ±å–œ é‡‘è›‡è³€æ­² æ¯›çµ¨é–€æ›\n",
      "å®˜æ–¹: é‡‘é—ª\n",
      "ç³»åˆ—: è¿·ä½ ZIMOMOç³»åˆ—2ä»£2019.08 (åŒ¹é…åº¦: 0.305, æ–¹å¼: TF-IDFåŒ¹é…)\n",
      "---\n",
      "å•†å“: ã€å¥½è©•éç™¾/èª ä¿¡è³£å®¶ã€‘THE MONSTER - å…¨æ–°æœªæ‹†ç›’æ©Ÿæ¬¾/æ‹†ç›’æœªæ‹†è¢‹æ¬¾/ç¤¼ç‰©é¦–é¸/å¿ƒåº•å¯†ç¢¼ç³»åˆ—/mini labubu 4.0/ A-M & N-Z ï¼ˆåƒ¹éŒ¢å„ªæƒ è«‹ç‡è©³æƒ…ï¼‰\n",
      "å®˜æ–¹: è‘¡è„\n",
      "ç³»åˆ—: æ°´æœç³»åˆ—2021.12 (åŒ¹é…åº¦: 0.218, æ–¹å¼: TF-IDFåŒ¹é…)\n",
      "---\n",
      "å•†å“: Pop Mart LABUBU å…¨æ–° è”æè“è“ ç›²ç›’ The Monsters Exciting Macaron Lychee Berry\n",
      "å®˜æ–¹: è”æè“è“\n",
      "ç³»åˆ—: å¿ƒåŠ¨é©¬å¡é¾™æªèƒ¶è„¸ç³»åˆ—2023.10 (åŒ¹é…åº¦: 0.541, æ–¹å¼: TF-IDFåŒ¹é…)\n",
      "---\n",
      "å•†å“: POPMART Labubu åå\n",
      "å®˜æ–¹: éšè—æ¬¾-æ£®æ—åå\n",
      "ç³»åˆ—: ç²¾çµæ£®æ—æ´¾å¯¹ç³»åˆ—2023.09 (åŒ¹é…åº¦: 0.481, æ–¹å¼: TF-IDFåŒ¹é…)\n",
      "---\n",
      "å•†å“: Labubu 4.0 mini éš±è— çˆ±å¿ƒæ„Ÿå˜†è™Ÿ\n",
      "å®˜æ–¹: çˆ±æ˜¯é™ªä¼´\n",
      "ç³»åˆ—: å¿ƒæ„¿æŒ‡å°–ç³»åˆ—2023.08 (åŒ¹é…åº¦: 0.258, æ–¹å¼: TF-IDFåŒ¹é…)\n",
      "---\n",
      "å•†å“: Labubuå‡Œæ™¨ä¸€ç‚¹\n",
      "å®˜æ–¹: Labubu\n",
      "ç³»åˆ—: Labubu å¿«ä¹ä¸‡åœ£èŠ‚æ´¾å¯¹ç³»åˆ—2024.10 (åŒ¹é…åº¦: 0.467, æ–¹å¼: TF-IDFåŒ¹é…)\n",
      "---\n",
      "å•†å“: The Monsters Space Adventures Labubu ç›²ç›’ï¼ˆå¥³å·«ï¼‰\n",
      "å®˜æ–¹: å¥³å·«\n",
      "ç³»åˆ—: æ€ªç‰©å˜‰å¹´åç³»åˆ—2019.10 (åŒ¹é…åº¦: 0.234, æ–¹å¼: TF-IDFåŒ¹é…)\n",
      "---\n",
      "å•†å“: Labubu 3.0 å‰æ–¹é«˜èƒ½ å¿ èª  é½Šç›’å¡\n",
      "å®˜æ–¹: å¿ è¯š\n",
      "ç³»åˆ—: å‰æ–¹é«˜èƒ½ç³»åˆ—-æªèƒ¶æ¯›ç»’æŒ‚ä»¶2025.04 (åŒ¹é…åº¦: 0.304, æ–¹å¼: TF-IDFåŒ¹é…)\n",
      "---\n",
      "å•†å“: POP MART The Monsters Labubu ååæ´¾å°\n",
      "å®˜æ–¹: éšè—æ¬¾-æ£®æ—åå\n",
      "ç³»åˆ—: ç²¾çµæ£®æ—æ´¾å¯¹ç³»åˆ—2023.09 (åŒ¹é…åº¦: 0.224, æ–¹å¼: TF-IDFåŒ¹é…)\n",
      "---\n",
      "\n",
      "=== æœªåŒ¹é…ç¤ºä¾‹ï¼ˆéœ€è¦æ”¹è¿›ï¼‰===\n",
      "æœªåŒ¹é…å•†å“: POP MART Almostç³»åˆ— Labubu éš±è—ç‰ˆ (æœ€é«˜åŒ¹é…åº¦: 0.15)\n",
      "æœªåŒ¹é…å•†å“: Labubu æœå‡è²¼ç´™ (æœ€é«˜åŒ¹é…åº¦: 0.13)\n",
      "æœªåŒ¹é…å•†å“: Labubu æ¨‚åœ’ç‰ˆ (æœ€é«˜åŒ¹é…åº¦: 0.104)\n",
      "æœªåŒ¹é…å•†å“: Popmart labubu 3.0ä¸€ç«¯ (æœ€é«˜åŒ¹é…åº¦: 0.111)\n",
      "æœªåŒ¹é…å•†å“: Popmartæ³¡æ³¡ç‘ªç‰¹ The monster labubu ç‘œçˆç³»åˆ—æ‰‹è¾¦ (æœ€é«˜åŒ¹é…åº¦: 0.131)\n",
      "\n",
      "âœ… æœ€ç»ˆåŒ¹é…ç»“æœå·²ä¿å­˜åˆ°: D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\matches_labubu.csv\n",
      "\n",
      "=== å„ç³»åˆ—å•†å“æ•°é‡ ===\n",
      "æ°´æœç³»åˆ—2021.12: 3547ä»¶\n",
      "Labubu å¿«ä¹ä¸‡åœ£èŠ‚æ´¾å¯¹ç³»åˆ—2024.10: 3475ä»¶\n",
      "POPCARä¹å›­ç¢°ç¢°è½¦ç³»åˆ—2022.12: 646ä»¶\n",
      "[æ–°] Labubu å¿ƒåº•ç§˜å¯†ç³»åˆ—2025.08: 570ä»¶\n",
      "å¯çˆ±ç§å®¶è½¦ç³»åˆ—2022.01: 537ä»¶\n",
      "æ€ªå‘³ä¾¿åˆ©åº—ç³»åˆ—2025.06: 421ä»¶\n",
      "æ…µæ‡’ç‘œä¼½ç³»åˆ—2024.08: 402ä»¶\n",
      "THE MONSTERS xæµ·ç»µå®å®ç³»åˆ—2021.04: 262ä»¶\n",
      "è™è·ƒæ–°æ˜¥ç³»åˆ—2022.01: 256ä»¶\n",
      "æ¨ªå±±å®Ma.K.ç³»åˆ—2023.04: 213ä»¶\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def scrape_labubu_directory():\n",
    "    \"\"\"\n",
    "    ä¸“é—¨çˆ¬å– labubu.directory å®˜æ–¹å›¾é‰´æ•°æ®\n",
    "    \"\"\"\n",
    "    base_url = \"https://labubu.directory\"\n",
    "    series_url = \"https://labubu.directory/zh/series\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        'Referer': 'https://labubu.directory/'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"æ­£åœ¨è®¿é—®ç³»åˆ—é¡µé¢...\")\n",
    "        response = requests.get(series_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # æŸ¥æ‰¾æ‰€æœ‰ç³»åˆ—é“¾æ¥\n",
    "        series_links = []\n",
    "        \n",
    "        # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥æ‰¾åˆ°ç³»åˆ—é“¾æ¥\n",
    "        selectors = [\n",
    "            'a[href*=\"/zh/series/\"]',\n",
    "            '.series-item a',\n",
    "            '.collection-item a',\n",
    "            '.grid a',\n",
    "            'a.card'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            links = soup.select(selector)\n",
    "            if links:\n",
    "                print(f\"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(links)} ä¸ªç³»åˆ—\")\n",
    "                series_links.extend(links)\n",
    "                break\n",
    "        \n",
    "        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´é€šç”¨çš„æ–¹æ³•\n",
    "        if not series_links:\n",
    "            all_links = soup.find_all('a', href=True)\n",
    "            series_links = [link for link in all_links if '/zh/series/' in link['href']]\n",
    "            print(f\"é€šè¿‡é€šç”¨æ–¹æ³•æ‰¾åˆ° {len(series_links)} ä¸ªç³»åˆ—\")\n",
    "        \n",
    "        # å»é‡å¹¶æ„å»ºå®Œæ•´URL\n",
    "        unique_series_links = []\n",
    "        seen_urls = set()\n",
    "        \n",
    "        for link in series_links:\n",
    "            href = link['href']\n",
    "            if href.startswith('/'):\n",
    "                full_url = urljoin(base_url, href)\n",
    "            else:\n",
    "                full_url = href\n",
    "            \n",
    "            if full_url not in seen_urls and '/zh/series/' in full_url:\n",
    "                seen_urls.add(full_url)\n",
    "                series_name = link.get_text(strip=True) or \"æœªçŸ¥ç³»åˆ—\"\n",
    "                unique_series_links.append({\n",
    "                    'url': full_url,\n",
    "                    'name': series_name\n",
    "                })\n",
    "        \n",
    "        print(f\"å»é‡åå¾—åˆ° {len(unique_series_links)} ä¸ªç³»åˆ—\")\n",
    "        \n",
    "        # çˆ¬å–æ¯ä¸ªç³»åˆ—çš„å•†å“\n",
    "        all_products = []\n",
    "        \n",
    "        for i, series in enumerate(unique_series_links, 1):\n",
    "            print(f\"æ­£åœ¨çˆ¬å–ç³»åˆ— {i}/{len(unique_series_links)}: {series['name']}\")\n",
    "            \n",
    "            try:\n",
    "                series_response = requests.get(series['url'], headers=headers)\n",
    "                series_response.raise_for_status()\n",
    "                series_soup = BeautifulSoup(series_response.content, 'html.parser')\n",
    "                \n",
    "                # æŸ¥æ‰¾ç³»åˆ—ä¸­çš„å•†å“\n",
    "                product_selectors = [\n",
    "                    '.product-card',\n",
    "                    '.item-card', \n",
    "                    '.character-card',\n",
    "                    '.grid > div',  # ç½‘æ ¼å¸ƒå±€ä¸­çš„é¡¹ç›®\n",
    "                    '[class*=\"card\"]'  # ä»»ä½•åŒ…å«cardçš„ç±»\n",
    "                ]\n",
    "                \n",
    "                products_in_series = []\n",
    "                for selector in product_selectors:\n",
    "                    products = series_soup.select(selector)\n",
    "                    if products:\n",
    "                        print(f\"  ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(products)} ä¸ªå•†å“\")\n",
    "                        products_in_series = products\n",
    "                        break\n",
    "                \n",
    "                # æå–æ¯ä¸ªå•†å“çš„ä¿¡æ¯\n",
    "                for product in products_in_series:\n",
    "                    try:\n",
    "                        # æå–å•†å“åç§°\n",
    "                        name_selectors = [\n",
    "                            '.product-name', '.character-name', '.name',\n",
    "                            'h3', 'h4', '.title', '[class*=\"name\"]',\n",
    "                            '.text-lg', '.font-bold'  # å¸¸è§çš„å¤§å­—ç²—ä½“æ–‡æœ¬\n",
    "                        ]\n",
    "                        \n",
    "                        product_name = None\n",
    "                        for name_selector in name_selectors:\n",
    "                            name_element = product.select_one(name_selector)\n",
    "                            if name_element:\n",
    "                                product_name = name_element.get_text(strip=True)\n",
    "                                break\n",
    "                        \n",
    "                        # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•è·å–æ•´ä¸ªå¡ç‰‡çš„æ–‡æœ¬\n",
    "                        if not product_name:\n",
    "                            product_name = product.get_text(strip=True)\n",
    "                            # æ¸…ç†æ–‡æœ¬ï¼Œåªå–å‰50ä¸ªå­—ç¬¦ä½œä¸ºåç§°\n",
    "                            if len(product_name) > 50:\n",
    "                                product_name = re.sub(r'\\s+', ' ', product_name[:50] + \"...\")\n",
    "                        \n",
    "                        # æå–å›¾ç‰‡\n",
    "                        img_selectors = ['img', '.product-image', '.character-image']\n",
    "                        img_url = None\n",
    "                        for img_selector in img_selectors:\n",
    "                            img_element = product.select_one(img_selector)\n",
    "                            if img_element and img_element.get('src'):\n",
    "                                img_src = img_element.get('src')\n",
    "                                if img_src.startswith('//'):\n",
    "                                    img_url = 'https:' + img_src\n",
    "                                elif img_src.startswith('/'):\n",
    "                                    img_url = urljoin(base_url, img_src)\n",
    "                                else:\n",
    "                                    img_url = img_src\n",
    "                                break\n",
    "                        \n",
    "                        if product_name and len(product_name) > 1:\n",
    "                            all_products.append({\n",
    "                                'å®˜æ–¹åç§°': product_name,\n",
    "                                'ç³»åˆ—åç§°': series['name'],\n",
    "                                'å›¾ç‰‡URL': img_url,\n",
    "                                'ç³»åˆ—URL': series['url']\n",
    "                            })\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"    æå–å•†å“æ—¶å‡ºé”™: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # ç¤¼è²Œæ€§å»¶è¿Ÿ\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"çˆ¬å–ç³»åˆ— {series['name']} æ—¶å‡ºé”™: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"æ€»å…±çˆ¬å–åˆ° {len(all_products)} ä¸ªå®˜æ–¹å•†å“\")\n",
    "        return all_products\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}\")\n",
    "        return []\n",
    "\n",
    "# æ–°å¢ï¼šç¹ç®€è½¬æ¢å‡½æ•°ï¼ˆåœ¨åŒ¹é…å‰ä½¿ç”¨ï¼‰\n",
    "def convert_traditional_to_simplified(text):\n",
    "    \"\"\"\n",
    "    å°†ç¹ä½“ä¸­æ–‡è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "        \n",
    "    trad_to_simp = {\n",
    "        'å¯å£å¯æ¨‚': 'å¯å£å¯ä¹', 'éª·é«': 'éª·é«…', 'è¬è–ç¯€': 'ä¸‡åœ£èŠ‚', 'è–èª•': 'åœ£è¯',\n",
    "        'å¾©æ´»ç¯€': 'å¤æ´»èŠ‚', 'éŸ³æ¨‚': 'éŸ³ä¹', 'åŠ‡å ´': 'å‰§åœº', 'å†’éšª': 'å†’é™©', 'æµ·æ´‹': 'æµ·æ´‹',\n",
    "        'æ£®æ—': 'æ£®æ—', 'å‹•ç‰©': 'åŠ¨ç‰©', 'ç«¥è©±': 'ç«¥è¯', 'é­”æ³•': 'é­”æ³•', 'å…¬ä¸»': 'å…¬ä¸»',\n",
    "        'ç‹å­': 'ç‹å­', 'é¨å£«': 'éª‘å£«', 'æ­¦å£«': 'æ­¦å£«', 'å¿è€…': 'å¿è€…', 'æ€ªç¸': 'æ€ªå…½',\n",
    "        'æé¾': 'æé¾™', 'å¤ªç©º': 'å¤ªç©º', 'å®‡å®™': 'å®‡å®™', 'æ˜Ÿæ˜Ÿ': 'æ˜Ÿæ˜Ÿ', 'æœˆäº®': 'æœˆäº®',\n",
    "        'å¤ªé™½': 'å¤ªé˜³', 'å½©è™¹': 'å½©è™¹', 'é›²æœµ': 'äº‘æœµ', 'é›¨æ»´': 'é›¨æ»´', 'é›ªèŠ±': 'é›ªèŠ±',\n",
    "        'ç¦®ç‰©': 'ç¤¼ç‰©', 'æ„›å¿ƒ': 'çˆ±å¿ƒ', 'å¤©ä½¿': 'å¤©ä½¿', 'é­”é¬¼': 'é­”é¬¼', 'ç²¾éˆ': 'ç²¾çµ',\n",
    "        'å¹½éˆ': 'å¹½çµ', 'å·«å¸«': 'å·«å¸ˆ', 'å—ç“œ': 'å—ç“œ', 'ç³–æœ': 'ç³–æœ', 'ç¦®æœ': 'ç¤¼æœ',\n",
    "        'çš‡å† ': 'çš‡å† ', 'å¯¶çŸ³': 'å®çŸ³', 'çç ': 'çç ', 'é‘½çŸ³': 'é’»çŸ³', 'é»ƒé‡‘': 'é»„é‡‘',\n",
    "        'ç™½éŠ€': 'ç™½é“¶', 'éŠ…ç‰Œ': 'é“œç‰Œ', 'çç›ƒ': 'å¥–æ¯', 'çç‰Œ': 'é‡‘ç‰Œ', 'å† è»': 'å† å†›',\n",
    "        'äºè»': 'äºšå†›', 'å­£è»': 'å­£å†›', 'æ¯”è³½': 'æ¯”èµ›', 'é‹å‹•': 'è¿åŠ¨', 'ç±ƒçƒ': 'ç¯®çƒ',\n",
    "        'è¶³çƒ': 'è¶³çƒ', 'ç¶²çƒ': 'ç½‘çƒ', 'æ£’çƒ': 'æ£’çƒ', 'æ¸¸æ³³': 'æ¸¸æ³³', 'æ»‘é›ª': 'æ»‘é›ª',\n",
    "        'è¡æµª': 'å†²æµª', 'æ½›æ°´': 'æ½œæ°´', 'é‡£é­š': 'é’“é±¼', 'éœ²ç‡Ÿ': 'éœ²è¥', 'æ—…è¡Œ': 'æ—…è¡Œ',\n",
    "        'ä¸€é»': 'ä¸€ç‚¹', 'å…©é»': 'ä¸¤ç‚¹', 'ä¸‰é»': 'ä¸‰ç‚¹', 'å››é»': 'å››ç‚¹', 'äº”é»': 'äº”ç‚¹'\n",
    "    }\n",
    "    \n",
    "    text_str = str(text)\n",
    "    for trad, simp in trad_to_simp.items():\n",
    "        text_str = text_str.replace(trad, simp)\n",
    "    return text_str\n",
    "\n",
    "def enhanced_text_matching(df_products, df_official):\n",
    "    \"\"\"\n",
    "    å¢å¼ºçš„æ–‡æœ¬åŒ¹é…ç®—æ³•ï¼Œä¸“é—¨é’ˆå¯¹Labubuå•†å“\n",
    "    \"\"\"\n",
    "    \n",
    "    def traditional_to_simplified(text):\n",
    "        \"\"\"\n",
    "        ç®€å•çš„ç¹ç®€è½¬æ¢å‡½æ•°\n",
    "        å¯ä»¥æ·»åŠ æ›´å¤šå¸¸è§çš„ç¹ç®€å¯¹åº”è¯\n",
    "        \"\"\"\n",
    "        trad_to_simp = {\n",
    "            'å¯å£å¯æ¨‚': 'å¯å£å¯ä¹',\n",
    "            'éª·é«': 'éª·é«…',\n",
    "            'è¬è–ç¯€': 'ä¸‡åœ£èŠ‚',\n",
    "            'è–èª•': 'åœ£è¯',\n",
    "            'å¾©æ´»ç¯€': 'å¤æ´»èŠ‚',\n",
    "            'éŸ³æ¨‚': 'éŸ³ä¹',\n",
    "            'åŠ‡å ´': 'å‰§åœº',\n",
    "            'å†’éšª': 'å†’é™©',\n",
    "            'æµ·æ´‹': 'æµ·æ´‹',\n",
    "            'æ£®æ—': 'æ£®æ—',\n",
    "            'å‹•ç‰©': 'åŠ¨ç‰©',\n",
    "            'ç«¥è©±': 'ç«¥è¯',\n",
    "            'é­”æ³•': 'é­”æ³•',\n",
    "            'å…¬ä¸»': 'å…¬ä¸»',\n",
    "            'ç‹å­': 'ç‹å­',\n",
    "            'é¨å£«': 'éª‘å£«',\n",
    "            'æ­¦å£«': 'æ­¦å£«',\n",
    "            'å¿è€…': 'å¿è€…',\n",
    "            'æ€ªç¸': 'æ€ªå…½',\n",
    "            'æé¾': 'æé¾™',\n",
    "            'å¤ªç©º': 'å¤ªç©º',\n",
    "            'å®‡å®™': 'å®‡å®™',\n",
    "            'æ˜Ÿæ˜Ÿ': 'æ˜Ÿæ˜Ÿ',\n",
    "            'æœˆäº®': 'æœˆäº®',\n",
    "            'å¤ªé™½': 'å¤ªé˜³',\n",
    "            'å½©è™¹': 'å½©è™¹',\n",
    "            'é›²æœµ': 'äº‘æœµ',\n",
    "            'é›¨æ»´': 'é›¨æ»´',\n",
    "            'é›ªèŠ±': 'é›ªèŠ±'\n",
    "        }\n",
    "        \n",
    "        for trad, simp in trad_to_simp.items():\n",
    "            text = text.replace(trad, simp)\n",
    "        return text\n",
    "    \n",
    "    def labubu_text_preprocess(text):\n",
    "        \"\"\"ä¸“é—¨é’ˆå¯¹Labubuå•†å“çš„æ–‡æœ¬é¢„å¤„ç†\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # ç¹ç®€è½¬æ¢\n",
    "        text = traditional_to_simplified(text)\n",
    "        \n",
    "        # ä¿ç•™å…³é”®ä¿¡æ¯ï¼šæ•°å­—ã€ä¸­æ–‡ã€è‹±æ–‡ã€ç©ºæ ¼ã€ç‚¹å·ï¼ˆç”¨äºç‰ˆæœ¬å·ï¼‰\n",
    "        # æ‰©å±•å­—ç¬¦é›†ï¼ŒåŒ…å«æ›´å¤šå¯èƒ½çš„ä¸­æ–‡æ ‡ç‚¹\n",
    "        text = re.sub(r'[^\\w\\s\\.\\Â·\\-\\u4e00-\\u9fff]', '', text)\n",
    "        \n",
    "        # å¤„ç†å¸¸è§çš„ç¬¦å·å˜ä½“\n",
    "        text = re.sub(r'[Â·ãƒ»]', '', text)  # ä¸­ç‚¹ç¬¦å·\n",
    "        \n",
    "        # Labubuç‰¹å®šçš„åœç”¨è¯ - å‡å°‘åœç”¨è¯æ•°é‡ï¼Œä¿ç•™æ›´å¤šå…³é”®è¯\n",
    "        labubu_stop_words = {\n",
    "            'pop', 'mart', 'labubu', 'the', 'monsters', 'ç³»åˆ—', 'æ¬¾', 'ç‰ˆ', \n",
    "            'æ­£ç‰ˆ', 'å…¨æ–°', 'æœªæ‹†', 'ç°è´§', 'åŒ…é‚®', 'ç‰¹ä»·', 'ä¼˜æƒ ', 'å‡ºå”®',\n",
    "            'è½¬è®©', 'åŒ…æ­£', 'ä¿è¯', 'æ­£å“', 'popmart', 'monster', 'æ‰‹åŠ',\n",
    "            'ç›²ç›’', 'æ¨¡å‹', 'ç©å…·', 'å…¬ä»”'\n",
    "        }\n",
    "        \n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in labubu_stop_words and len(word) > 0]\n",
    "        \n",
    "        # å¦‚æœè¿‡æ»¤åä¸ºç©ºï¼Œè¿”å›åŸå§‹æ–‡æœ¬çš„ç®€åŒ–ç‰ˆæœ¬\n",
    "        if not filtered_words:\n",
    "            # åªç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™æ‰€æœ‰è¯æ±‡\n",
    "            simple_text = re.sub(r'[^\\w\\s\\u4e00-\\u9fff]', '', str(text))\n",
    "            return simple_text.strip()\n",
    "        \n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    print(\"æ­£åœ¨é¢„å¤„ç†æ–‡æœ¬...\")\n",
    "    df_products['æ¸…æ´—åç§°'] = df_products['å•†å“åç§°'].apply(labubu_text_preprocess)\n",
    "    df_official['æ¸…æ´—å®˜æ–¹åç§°'] = df_official['å®˜æ–¹åç§°'].apply(labubu_text_preprocess)\n",
    "    \n",
    "    # æ·»åŠ è°ƒè¯•ä¿¡æ¯\n",
    "    print(\"\\n=== é¢„å¤„ç†ç¤ºä¾‹ ===\")\n",
    "    for i, (orig, cleaned) in enumerate(zip(df_products['å•†å“åç§°'].head(5), df_products['æ¸…æ´—åç§°'].head(5))):\n",
    "        print(f\"åŸå§‹: {orig} -> æ¸…æ´—: {cleaned}\")\n",
    "    \n",
    "    # ä½¿ç”¨å­—ç¬¦çº§åˆ«çš„TF-IDFï¼Œæ›´é€‚åˆçŸ­æ–‡æœ¬åŒ¹é…\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer='char_wb',  # å­—ç¬¦çº§åˆ«çš„n-gramï¼Œå°Šé‡å•è¯è¾¹ç•Œ\n",
    "        ngram_range=(1, 3),  # è°ƒæ•´ä¸º1åˆ°3ä¸ªå­—ç¬¦çš„n-gramï¼Œæ›´å¥½åœ°å¤„ç†çŸ­è¯\n",
    "        min_df=1,\n",
    "        max_features=10000,\n",
    "        lowercase=False  # ä¸è½¬æ¢ä¸ºå°å†™ï¼Œä¿ç•™åŸå§‹å¤§å°å†™ä¿¡æ¯ï¼ˆå› ä¸ºæˆ‘ä»¬å·²ç»ç»Ÿä¸€è½¬å°å†™äº†ï¼‰\n",
    "    )\n",
    "    \n",
    "    # åˆå¹¶æ‰€æœ‰æ–‡æœ¬è¿›è¡Œè®­ç»ƒ\n",
    "    all_texts = list(df_products['æ¸…æ´—åç§°']) + list(df_official['æ¸…æ´—å®˜æ–¹åç§°'])\n",
    "    vectorizer.fit(all_texts)\n",
    "    \n",
    "    products_tfidf = vectorizer.transform(df_products['æ¸…æ´—åç§°'])\n",
    "    official_tfidf = vectorizer.transform(df_official['æ¸…æ´—å®˜æ–¹åç§°'])\n",
    "    \n",
    "    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ\n",
    "    similarity_matrix = cosine_similarity(products_tfidf, official_tfidf)\n",
    "    \n",
    "    # æ‰¾åˆ°æœ€ä½³åŒ¹é…\n",
    "    matches = []\n",
    "    for i, product_row in df_products.iterrows():\n",
    "        best_match_idx = np.argmax(similarity_matrix[i])\n",
    "        best_score = similarity_matrix[i][best_match_idx]\n",
    "        \n",
    "        # åŠ¨æ€é˜ˆå€¼ï¼šå¯¹äºåŒ…å«å…³é”®å­—çš„å•†å“ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼\n",
    "        product_text = product_row['æ¸…æ´—åç§°']\n",
    "        has_keywords = any(keyword in product_text for keyword in ['å¯å£å¯ä¹', 'coca', 'cola', 'å¯ä¹'])\n",
    "        \n",
    "        threshold = 0.1 if has_keywords else 0.15  # å¯¹å…³é”®è¯å•†å“ä½¿ç”¨æ›´ä½çš„é˜ˆå€¼\n",
    "        \n",
    "        if best_score > threshold:\n",
    "            official_match = df_official.iloc[best_match_idx]\n",
    "            matches.append({\n",
    "                'å•†å“åç§°': product_row['å•†å“åç§°'],\n",
    "                'åŒ¹é…å®˜æ–¹åç§°': official_match['å®˜æ–¹åç§°'],\n",
    "                'åŒ¹é…ç³»åˆ—': official_match['ç³»åˆ—åç§°'],\n",
    "                'åŒ¹é…å›¾ç‰‡URL': official_match.get('å›¾ç‰‡URL', ''),\n",
    "                'åŒ¹é…åº¦': round(best_score, 3),\n",
    "                'æ˜¯å¦åŒ¹é…æˆåŠŸ': True\n",
    "            })\n",
    "        else:\n",
    "            matches.append({\n",
    "                'å•†å“åç§°': product_row['å•†å“åç§°'],\n",
    "                'åŒ¹é…å®˜æ–¹åç§°': 'æœªåŒ¹é…åˆ°å®˜æ–¹åç§°',\n",
    "                'åŒ¹é…ç³»åˆ—': 'æœªåˆ†ç±»',\n",
    "                'åŒ¹é…å›¾ç‰‡URL': '',\n",
    "                'åŒ¹é…åº¦': round(best_score, 3),\n",
    "                'æ˜¯å¦åŒ¹é…æˆåŠŸ': False\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "# æ–°å¢å‡½æ•°ï¼šåŸºäºå…³é”®è¯çš„è¡¥å……åŒ¹é…\n",
    "def keyword_based_matching(df_final, df_official):\n",
    "    \"\"\"\n",
    "    å¯¹TF-IDFæœªåŒ¹é…æˆåŠŸçš„å•†å“è¿›è¡ŒåŸºäºå…³é”®è¯çš„è¡¥å……åŒ¹é…\n",
    "    \"\"\"\n",
    "    # æ„å»ºå…³é”®è¯åˆ°ç³»åˆ—çš„æ˜ å°„\n",
    "    keyword_to_series = {}\n",
    "    for _, row in df_official.iterrows():\n",
    "        series_name = row['ç³»åˆ—åç§°']\n",
    "        official_name = row['å®˜æ–¹åç§°']\n",
    "        \n",
    "        # ä»ç³»åˆ—åç§°å’Œå®˜æ–¹åç§°ä¸­æå–å…³é”®è¯\n",
    "        keywords = set()\n",
    "        \n",
    "        # ç³»åˆ—åç§°å…³é”®è¯\n",
    "        series_words = re.findall(r'[\\u4e00-\\u9fff]+|[a-zA-Z]+', str(series_name))\n",
    "        keywords.update([word.lower() for word in series_words if len(word) > 1])\n",
    "        \n",
    "        # å®˜æ–¹åç§°å…³é”®è¯\n",
    "        official_words = re.findall(r'[\\u4e00-\\u9fff]+|[a-zA-Z]+', str(official_name))\n",
    "        keywords.update([word.lower() for word in official_words if len(word) > 1])\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            if keyword not in ['labubu', 'ç³»åˆ—', 'æ¬¾', 'ç‰ˆ']:  # æ’é™¤é€šç”¨è¯\n",
    "                if keyword not in keyword_to_series:\n",
    "                    keyword_to_series[keyword] = []\n",
    "                if series_name not in keyword_to_series[keyword]:\n",
    "                    keyword_to_series[keyword].append(series_name)\n",
    "    \n",
    "    print(f\"æ„å»ºäº† {len(keyword_to_series)} ä¸ªå…³é”®è¯åˆ°ç³»åˆ—çš„æ˜ å°„\")\n",
    "    \n",
    "    # å¯¹æœªåŒ¹é…çš„å•†å“è¿›è¡Œå…³é”®è¯åŒ¹é…\n",
    "    unmatched_df = df_final[df_final['æ˜¯å¦åŒ¹é…æˆåŠŸ'] == False].copy()\n",
    "    matched_count = 0\n",
    "    \n",
    "    for idx, row in unmatched_df.iterrows():\n",
    "        product_name = str(row['å•†å“åç§°']).lower()\n",
    "        product_cleaned = str(row.get('æ¸…æ´—åç§°', '')).lower()\n",
    "        \n",
    "        best_series = None\n",
    "        best_score = 0\n",
    "        \n",
    "        # æ£€æŸ¥æ‰€æœ‰å…³é”®è¯\n",
    "        for keyword, series_list in keyword_to_series.items():\n",
    "            if len(keyword) < 2:  # è·³è¿‡å¤ªçŸ­çš„å…³é”®è¯\n",
    "                continue\n",
    "                \n",
    "            # æ£€æŸ¥å•†å“åç§°ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯\n",
    "            if keyword in product_name or keyword in product_cleaned:\n",
    "                for series in series_list:\n",
    "                    # ç®€å•çš„åŒ…å«åŒ¹é…\n",
    "                    score = len(keyword) / len(product_name)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_series = series\n",
    "        \n",
    "        # å¦‚æœæ‰¾åˆ°åŒ¹é…ä¸”ç½®ä¿¡åº¦è¶³å¤Ÿé«˜\n",
    "        if best_series and best_score > 0.3:\n",
    "            # æ‰¾åˆ°å¯¹åº”çš„å®˜æ–¹è®°å½•\n",
    "            official_match = df_official[df_official['ç³»åˆ—åç§°'] == best_series].iloc[0]\n",
    "            df_final.loc[idx, 'åŒ¹é…å®˜æ–¹åç§°'] = official_match['å®˜æ–¹åç§°']\n",
    "            df_final.loc[idx, 'åŒ¹é…ç³»åˆ—'] = best_series\n",
    "            df_final.loc[idx, 'åŒ¹é…å›¾ç‰‡URL'] = official_match.get('å›¾ç‰‡URL', '')\n",
    "            df_final.loc[idx, 'åŒ¹é…åº¦'] = round(best_score, 3)\n",
    "            df_final.loc[idx, 'æ˜¯å¦åŒ¹é…æˆåŠŸ'] = True\n",
    "            df_final.loc[idx, 'åŒ¹é…æ–¹å¼'] = 'å…³é”®è¯åŒ¹é…'\n",
    "            matched_count += 1\n",
    "    \n",
    "    print(f\"é€šè¿‡å…³é”®è¯åŒ¹é…æˆåŠŸåŒ¹é…äº† {matched_count} ä¸ªå•†å“\")\n",
    "    return df_final\n",
    "\n",
    "# æ–°å¢ï¼šå‡Œæ™¨å…³é”®è¯åå¤„ç†å‡½æ•°\n",
    "def post_process_morning_keywords(df_final):\n",
    "    \"\"\"\n",
    "    åœ¨åŒ¹é…å®Œæˆåï¼Œæ£€æŸ¥æœªåŒ¹é…ä½†åŒ…å«å‡Œæ™¨å…³é”®è¯çš„å•†å“ï¼Œå°†å…¶åˆ†ç±»ä¸ºå‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\n",
    "    \"\"\"\n",
    "    # æŸ¥æ‰¾æœªåŒ¹é…æˆåŠŸä½†åŒ…å«å‡Œæ™¨å…³é”®è¯çš„å•†å“\n",
    "    morning_keywords = ['å‡Œæ™¨', '1am', '1:00', '01:00', 'ä¸€é»', 'ä¸€ç‚¹']\n",
    "    pattern = '|'.join(morning_keywords)\n",
    "    \n",
    "    unmatched_with_morning = df_final[\n",
    "        (df_final['æ˜¯å¦åŒ¹é…æˆåŠŸ'] == False) & \n",
    "        (df_final['å•†å“åç§°'].str.contains(pattern, case=False, na=False))\n",
    "    ]\n",
    "    \n",
    "    if not unmatched_with_morning.empty:\n",
    "        print(f\"å‘ç° {len(unmatched_with_morning)} ä¸ªæœªåŒ¹é…ä½†åŒ…å«å‡Œæ™¨å…³é”®è¯çš„å•†å“ï¼Œå°†å…¶åˆ†ç±»ä¸ºå‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\")\n",
    "        \n",
    "        for idx in unmatched_with_morning.index:\n",
    "            df_final.loc[idx, 'åŒ¹é…ç³»åˆ—'] = 'å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—'\n",
    "            df_final.loc[idx, 'åŒ¹é…å®˜æ–¹åç§°'] = 'å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—ï¼ˆåå¤„ç†åŒ¹é…ï¼‰'\n",
    "            df_final.loc[idx, 'åŒ¹é…åº¦'] = 1.0\n",
    "            df_final.loc[idx, 'æ˜¯å¦åŒ¹é…æˆåŠŸ'] = True\n",
    "            if 'åŒ¹é…æ–¹å¼' in df_final.columns:\n",
    "                df_final.loc[idx, 'åŒ¹é…æ–¹å¼'] = 'å‡Œæ™¨å…³é”®è¯åå¤„ç†'\n",
    "            \n",
    "            print(f\"âœ… åå¤„ç†åŒ¹é…: {df_final.loc[idx, 'å•†å“åç§°']} -> å‡Œæ™¨ä¸€ç‚¹ç³»åˆ—\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# ä¸»ç¨‹åº\n",
    "def main():\n",
    "    # è¯»å–å•†å“æ•°æ®\n",
    "    df_products = pd.read_csv(r\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\cleaned_labubu.csv\")\n",
    "    print(f\"åŠ è½½äº† {len(df_products)} ä¸ªå•†å“æ•°æ®\")\n",
    "    \n",
    "    # æ–°å¢ï¼šåœ¨åŒ¹é…å‰å°†å•†å“åç§°åˆ—ç”±ç¹ä½“è½¬æ¢ä¸ºç®€ä½“\n",
    "    print(\"æ­£åœ¨å°†å•†å“åç§°ç”±ç¹ä½“è½¬æ¢ä¸ºç®€ä½“...\")\n",
    "    df_products['å•†å“åç§°'] = df_products['å•†å“åç§°'].apply(convert_traditional_to_simplified)\n",
    "    print(\"âœ… å•†å“åç§°ç¹ç®€è½¬æ¢å®Œæˆ\")\n",
    "    \n",
    "    # æ˜¾ç¤ºè½¬æ¢åçš„ç¤ºä¾‹\n",
    "    print(\"\\n=== ç¹ç®€è½¬æ¢ç¤ºä¾‹ ===\")\n",
    "    for i, name in enumerate(df_products['å•†å“åç§°'].head(5)):\n",
    "        print(f\"ç¤ºä¾‹ {i+1}: {name}\")\n",
    "    \n",
    "    # çˆ¬å–å®˜æ–¹å›¾é‰´æ•°æ®\n",
    "    print(\"å¼€å§‹çˆ¬å–å®˜æ–¹å›¾é‰´æ•°æ®...\")\n",
    "    official_products = scrape_labubu_directory()\n",
    "    \n",
    "    if not official_products:\n",
    "        print(\"æœªèƒ½çˆ¬å–åˆ°å®˜æ–¹å›¾é‰´æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç½‘ç«™ç»“æ„\")\n",
    "        return\n",
    "    \n",
    "    # åˆ›å»ºå®˜æ–¹å•†å“DataFrame\n",
    "    df_official = pd.DataFrame(official_products)\n",
    "    \n",
    "    # å»é‡\n",
    "    df_official = df_official.drop_duplicates(subset=['å®˜æ–¹åç§°'])\n",
    "    print(f\"å»é‡åå®˜æ–¹å•†å“æ•°é‡: {len(df_official)}\")\n",
    "    \n",
    "    # ä¿å­˜å®˜æ–¹å›¾é‰´æ•°æ®\n",
    "    official_save_path = r\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\catalog_labubu.csv\"\n",
    "    df_official.to_csv(official_save_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… å®˜æ–¹å›¾é‰´æ•°æ®å·²ä¿å­˜åˆ°: {official_save_path}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºä¸€äº›å®˜æ–¹å•†å“ç¤ºä¾‹\n",
    "    print(\"\\n=== å®˜æ–¹å•†å“ç¤ºä¾‹ ===\")\n",
    "    for i, row in df_official.head(10).iterrows():\n",
    "        print(f\"{i+1}. {row['å®˜æ–¹åç§°']} - {row['ç³»åˆ—åç§°']}\")\n",
    "    \n",
    "    # è¿›è¡ŒåŒ¹é…\n",
    "    print(\"\\næ­£åœ¨è¿›è¡Œå•†å“åŒ¹é…...\")\n",
    "    df_matches = enhanced_text_matching(df_products, df_official)\n",
    "    \n",
    "    # åˆå¹¶ç»“æœ\n",
    "    df_final = df_products.merge(df_matches, on='å•†å“åç§°', how='left')\n",
    "    \n",
    "    # æ·»åŠ åŒ¹é…æ–¹å¼åˆ—\n",
    "    df_final['åŒ¹é…æ–¹å¼'] = 'TF-IDFåŒ¹é…'\n",
    "    \n",
    "    # è¿›è¡Œå…³é”®è¯è¡¥å……åŒ¹é…\n",
    "    print(\"\\næ­£åœ¨è¿›è¡Œå…³é”®è¯è¡¥å……åŒ¹é…...\")\n",
    "    df_final = keyword_based_matching(df_final, df_official)\n",
    "    \n",
    "    # æ–°å¢ï¼šå‡Œæ™¨å…³é”®è¯åå¤„ç†\n",
    "    print(\"\\næ­£åœ¨è¿›è¡Œå‡Œæ™¨å…³é”®è¯åå¤„ç†...\")\n",
    "    df_final = post_process_morning_keywords(df_final)\n",
    "    \n",
    "    # åˆ†æåŒ¹é…ç»“æœ\n",
    "    match_stats = df_final['æ˜¯å¦åŒ¹é…æˆåŠŸ'].value_counts()\n",
    "    print(f\"\\n=== åŒ¹é…ç»“æœç»Ÿè®¡ ===\")\n",
    "    print(f\"æˆåŠŸåŒ¹é…: {match_stats.get(True, 0)} ä»¶å•†å“\")\n",
    "    print(f\"æœªæˆåŠŸåŒ¹é…: {match_stats.get(False, 0)} ä»¶å•†å“\")\n",
    "    if len(df_final) > 0:\n",
    "        success_rate = match_stats.get(True, 0) / len(df_final) * 100\n",
    "        print(f\"åŒ¹é…æˆåŠŸç‡: {success_rate:.1f}%\")\n",
    "    \n",
    "    # æ˜¾ç¤ºåŒ¹é…æ–¹å¼ç»Ÿè®¡\n",
    "    if 'åŒ¹é…æ–¹å¼' in df_final.columns:\n",
    "        method_stats = df_final[df_final['æ˜¯å¦åŒ¹é…æˆåŠŸ'] == True]['åŒ¹é…æ–¹å¼'].value_counts()\n",
    "        print(f\"\\n=== åŒ¹é…æ–¹å¼ç»Ÿè®¡ ===\")\n",
    "        for method, count in method_stats.items():\n",
    "            print(f\"{method}: {count}ä»¶\")\n",
    "    \n",
    "    # æ˜¾ç¤ºåŒ¹é…ç¤ºä¾‹\n",
    "    print(\"\\n=== æˆåŠŸåŒ¹é…ç¤ºä¾‹ ===\")\n",
    "    successful_matches = df_final[df_final['æ˜¯å¦åŒ¹é…æˆåŠŸ'] == True].head(10)\n",
    "    for _, row in successful_matches.iterrows():\n",
    "        match_method = row.get('åŒ¹é…æ–¹å¼', 'TF-IDFåŒ¹é…')\n",
    "        print(f\"å•†å“: {row['å•†å“åç§°']}\")\n",
    "        print(f\"å®˜æ–¹: {row['åŒ¹é…å®˜æ–¹åç§°']}\")\n",
    "        print(f\"ç³»åˆ—: {row['åŒ¹é…ç³»åˆ—']} (åŒ¹é…åº¦: {row['åŒ¹é…åº¦']}, æ–¹å¼: {match_method})\")\n",
    "        print(\"---\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæœªåŒ¹é…ç¤ºä¾‹ï¼ˆç”¨äºæ”¹è¿›ç®—æ³•ï¼‰\n",
    "    print(\"\\n=== æœªåŒ¹é…ç¤ºä¾‹ï¼ˆéœ€è¦æ”¹è¿›ï¼‰===\")\n",
    "    failed_matches = df_final[df_final['æ˜¯å¦åŒ¹é…æˆåŠŸ'] == False].head(5)\n",
    "    for _, row in failed_matches.iterrows():\n",
    "        print(f\"æœªåŒ¹é…å•†å“: {row['å•†å“åç§°']} (æœ€é«˜åŒ¹é…åº¦: {row['åŒ¹é…åº¦']})\")\n",
    "    \n",
    "    # ä¿å­˜æœ€ç»ˆç»“æœ\n",
    "    final_save_path = r\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\matches_labubu.csv\"\n",
    "    df_final.to_csv(final_save_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nâœ… æœ€ç»ˆåŒ¹é…ç»“æœå·²ä¿å­˜åˆ°: {final_save_path}\")\n",
    "    \n",
    "    # ç”Ÿæˆç³»åˆ—ç»Ÿè®¡\n",
    "    if not df_final[df_final['æ˜¯å¦åŒ¹é…æˆåŠŸ'] == True].empty:\n",
    "        series_stats = df_final[df_final['æ˜¯å¦åŒ¹é…æˆåŠŸ'] == True]['åŒ¹é…ç³»åˆ—'].value_counts()\n",
    "        print(f\"\\n=== å„ç³»åˆ—å•†å“æ•°é‡ ===\")\n",
    "        for series, count in series_stats.head(10).items():\n",
    "            print(f\"{series}: {count}ä»¶\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69848eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸåŠ è½½å•†å“æ•°æ®ï¼Œå…± 13962 æ¡\n",
      "æ•°æ®åŒ…å«åˆ—: ['å•†å“åç§°', 'ä»·æ ¼', 'å‘å¸ƒç”¨æˆ·', 'å‘å¸ƒæ—¶é—´', 'å•†å“é“¾æ¥', 'çˆ¬å–æ—¶é—´', 'æ¸…æ´—åç§°', 'åŒ¹é…å®˜æ–¹åç§°', 'åŒ¹é…ç³»åˆ—', 'åŒ¹é…å›¾ç‰‡URL', 'åŒ¹é…åº¦', 'æ˜¯å¦åŒ¹é…æˆåŠŸ', 'åŒ¹é…æ–¹å¼']\n",
      "\n",
      "=== å¼€å§‹æ•°æ®å»é‡ ===\n",
      "å»é‡å‰æ•°æ®é‡: 13962\n",
      "ç”¨äºå»é‡çš„åˆ—: ['å•†å“åç§°', 'ä»·æ ¼', 'å‘å¸ƒç”¨æˆ·', 'å‘å¸ƒæ—¶é—´', 'å•†å“é“¾æ¥', 'çˆ¬å–æ—¶é—´']\n",
      "å‘ç° 11919 æ¡é‡å¤æ•°æ®\n",
      "\n",
      "é‡å¤æ•°æ®ç¤ºä¾‹:\n",
      "ç¤ºä¾‹ 1:\n",
      "  å•†å“åç§°: POP MART The Monsters Labubu ååæ´¾å°\n",
      "  ä»·æ ¼: HK$120\n",
      "  å‘å¸ƒç”¨æˆ·: applepiie_\n",
      "  å‘å¸ƒæ—¶é—´: 39 åˆ†é˜å‰\n",
      "  å•†å“é“¾æ¥: https://www.carousell.com.hk/p/pop-mart-the-monsters-labubu-%E5%9D%90%E5%9D%90%E6%B4%BE%E5%B0%8D-1397749805/?t-id=Dg8pv0XwWo_1763803800169&t-referrer_browse_type=search_results&t-referrer_page_type=search&t-referrer_request_id=GFPyBKWWGNFP63Cq&t-referrer_search_query=labubu&t-referrer_sort_by=popular&t-tap_index=12\n",
      "  çˆ¬å–æ—¶é—´: 2025-11-22 17:31:15\n",
      "  ---\n",
      "ç¤ºä¾‹ 2:\n",
      "  å•†å“åç§°: POP MART The Monsters Labubu ååæ´¾å°\n",
      "  ä»·æ ¼: HK$120\n",
      "  å‘å¸ƒç”¨æˆ·: applepiie_\n",
      "  å‘å¸ƒæ—¶é—´: 39 åˆ†é˜å‰\n",
      "  å•†å“é“¾æ¥: https://www.carousell.com.hk/p/pop-mart-the-monsters-labubu-%E5%9D%90%E5%9D%90%E6%B4%BE%E5%B0%8D-1397749805/?t-id=Dg8pv0XwWo_1763803800169&t-referrer_browse_type=search_results&t-referrer_page_type=search&t-referrer_request_id=GFPyBKWWGNFP63Cq&t-referrer_search_query=labubu&t-referrer_sort_by=popular&t-tap_index=12\n",
      "  çˆ¬å–æ—¶é—´: 2025-11-22 17:31:15\n",
      "  ---\n",
      "ç¤ºä¾‹ 3:\n",
      "  å•†å“åç§°: POP MART LABUBU Why So Serious Twinkle Twinkle å…¬ä»”\n",
      "  ä»·æ ¼: HK$130\n",
      "  å‘å¸ƒç”¨æˆ·: verasin1289589\n",
      "  å‘å¸ƒæ—¶é—´: 52 åˆ†é˜å‰\n",
      "  å•†å“é“¾æ¥: https://www.carousell.com.hk/p/pop-mart-labubu-why-so-serious-twinkle-twinkle-%E5%85%AC%E4%BB%94-1405609570/?t-id=Dg8pv0XwWo_1763803800169&t-referrer_browse_type=search_results&t-referrer_page_type=search&t-referrer_request_id=GFPyBKWWGNFP63Cq&t-referrer_search_query=labubu&t-referrer_sort_by=popular&t-tap_index=26\n",
      "  çˆ¬å–æ—¶é—´: 2025-11-22 17:31:15\n",
      "  ---\n",
      "\n",
      "å»é‡åæ•°æ®é‡: 6684\n",
      "ç§»é™¤çš„é‡å¤æ•°æ®: 7278 æ¡\n",
      "å»é‡ç‡: 52.13%\n",
      "=== å»é‡å®Œæˆ ===\n",
      "\n",
      "=== å¼€å§‹è¿‡æ»¤æœªåŒ¹é…æ•°æ® ===\n",
      "è¿‡æ»¤å‰æ•°æ®é‡: 6684 æ¡\n",
      "æœªåŒ¹é…æ•°æ®é‡ï¼ˆåŒ¹é…ç³»åˆ—ä¸º ['æœªåŒ¹é…', 'æœªåˆ†ç±»', 'æœªåŒ¹é…åˆ°ç³»åˆ—', 'æœªåŒ¹é…åˆ°å®˜æ–¹åç§°']ï¼‰: 371 æ¡\n",
      "è¿‡æ»¤åæ•°æ®é‡: 6313 æ¡\n",
      "ç§»é™¤çš„æœªåŒ¹é…æ•°æ®: 371 æ¡\n",
      "=== è¿‡æ»¤æœªåŒ¹é…æ•°æ®å®Œæˆ ===\n",
      "\n",
      "âœ… æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°: D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\matched_labubu.csv\n",
      "âœ… æœ€ç»ˆæœ‰æ•ˆæ•°æ®é‡: 6313 æ¡ï¼ˆå»é‡+è¿‡æ»¤æœªåŒ¹é…åï¼‰\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_duplicate_data(df):\n",
    "    \"\"\"\n",
    "    å‰”é™¤å•†å“åç§°ã€ä»·æ ¼ã€å‘å¸ƒç”¨æˆ·ã€å‘å¸ƒæ—¶é—´ã€å•†å“é“¾æ¥å’Œçˆ¬å–æ—¶é—´å®Œå…¨ä¸€æ ·çš„æ•°æ®\n",
    "    \"\"\"\n",
    "    print(\"\\n=== å¼€å§‹æ•°æ®å»é‡ ===\")\n",
    "    original_count = len(df)\n",
    "    print(f\"å»é‡å‰æ•°æ®é‡: {original_count}\")\n",
    "    \n",
    "    # å®šä¹‰å»é‡çš„åˆ—\n",
    "    duplicate_columns = ['å•†å“åç§°', 'ä»·æ ¼', 'å‘å¸ƒç”¨æˆ·', 'å‘å¸ƒæ—¶é—´', 'å•†å“é“¾æ¥', 'çˆ¬å–æ—¶é—´']\n",
    "    \n",
    "    # æ£€æŸ¥å“ªäº›åˆ—å®é™…å­˜åœ¨äºDataFrameä¸­\n",
    "    available_columns = [col for col in duplicate_columns if col in df.columns]\n",
    "    print(f\"ç”¨äºå»é‡çš„åˆ—: {available_columns}\")\n",
    "    \n",
    "    # å®¹é”™ï¼šå¦‚æœæ²¡æœ‰å¯ç”¨å»é‡åˆ—ï¼Œç›´æ¥è¿”å›åŸå§‹æ•°æ®\n",
    "    if not available_columns:\n",
    "        print(\"âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„å»é‡åˆ—ï¼Œè¿”å›åŸå§‹æ•°æ®\")\n",
    "        return df\n",
    "    \n",
    "    # æ£€æŸ¥é‡å¤æ•°æ®\n",
    "    duplicate_rows = df[df.duplicated(subset=available_columns, keep=False)]\n",
    "    if not duplicate_rows.empty:\n",
    "        print(f\"å‘ç° {len(duplicate_rows)} æ¡é‡å¤æ•°æ®\")\n",
    "        print(\"\\né‡å¤æ•°æ®ç¤ºä¾‹:\")\n",
    "        for i, (idx, row) in enumerate(duplicate_rows.head(3).iterrows()):\n",
    "            print(f\"ç¤ºä¾‹ {i+1}:\")\n",
    "            for col in available_columns:\n",
    "                print(f\"  {col}: {row[col]}\")\n",
    "            print(\"  ---\")\n",
    "    else:\n",
    "        print(\"âœ… æœªå‘ç°é‡å¤æ•°æ®\")\n",
    "    \n",
    "    # æ‰§è¡Œå»é‡ï¼ˆä¿ç•™ç¬¬ä¸€æ¡è®°å½•ï¼‰\n",
    "    df_deduplicated = df.drop_duplicates(subset=available_columns, keep='first')\n",
    "    final_count = len(df_deduplicated)\n",
    "    \n",
    "    # è¾“å‡ºå»é‡ç»Ÿè®¡\n",
    "    removed_count = original_count - final_count\n",
    "    print(f\"\\nå»é‡åæ•°æ®é‡: {final_count}\")\n",
    "    print(f\"ç§»é™¤çš„é‡å¤æ•°æ®: {removed_count} æ¡\")\n",
    "    print(f\"å»é‡ç‡: {((removed_count / original_count) * 100):.2f}%\" if original_count > 0 else \"0.00%\")\n",
    "    print(\"=== å»é‡å®Œæˆ ===\")\n",
    "    \n",
    "    return df_deduplicated\n",
    "\n",
    "def filter_unmatched_data(df):\n",
    "    \"\"\"\n",
    "    åˆ é™¤åŒ¹é…ç³»åˆ—ä¸ºã€ŒæœªåŒ¹é…ã€æˆ–ã€Œæœªåˆ†ç±»ã€çš„æ•°æ®ï¼ˆå¯æ ¹æ®å®é™…å­—æ®µå€¼è°ƒæ•´ï¼‰\n",
    "    \"\"\"\n",
    "    print(\"\\n=== å¼€å§‹è¿‡æ»¤æœªåŒ¹é…æ•°æ® ===\")\n",
    "    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ã€ŒåŒ¹é…ç³»åˆ—ã€åˆ—\n",
    "    if 'åŒ¹é…ç³»åˆ—' not in df.columns:\n",
    "        print(\"âš ï¸  æ•°æ®ä¸­æœªæ‰¾åˆ°ã€åŒ¹é…ç³»åˆ—ã€åˆ—ï¼Œè·³è¿‡è¿‡æ»¤æ­¥éª¤\")\n",
    "        return df\n",
    "    \n",
    "    # å®šä¹‰æœªåŒ¹é…çš„å…³é”®è¯ï¼ˆæ ¹æ®å®é™…æ•°æ®è°ƒæ•´ï¼Œå¸¸è§å€¼ï¼šæœªåŒ¹é…ã€æœªåˆ†ç±»ã€æœªåŒ¹é…åˆ°ç³»åˆ—ï¼‰\n",
    "    unmatched_keywords = ['æœªåŒ¹é…', 'æœªåˆ†ç±»', 'æœªåŒ¹é…åˆ°ç³»åˆ—', 'æœªåŒ¹é…åˆ°å®˜æ–¹åç§°']\n",
    "    \n",
    "    # ç»Ÿè®¡æœªåŒ¹é…æ•°æ®é‡\n",
    "    unmatched_mask = df['åŒ¹é…ç³»åˆ—'].isin(unmatched_keywords)\n",
    "    unmatched_count = unmatched_mask.sum()\n",
    "    total_count = len(df)\n",
    "    \n",
    "    print(f\"è¿‡æ»¤å‰æ•°æ®é‡: {total_count} æ¡\")\n",
    "    print(f\"æœªåŒ¹é…æ•°æ®é‡ï¼ˆåŒ¹é…ç³»åˆ—ä¸º {unmatched_keywords}ï¼‰: {unmatched_count} æ¡\")\n",
    "    \n",
    "    # è¿‡æ»¤æœªåŒ¹é…æ•°æ®ï¼ˆä¿ç•™åŒ¹é…æˆåŠŸçš„æ•°æ®ï¼‰\n",
    "    df_filtered = df[~unmatched_mask].reset_index(drop=True)\n",
    "    filtered_count = len(df_filtered)\n",
    "    \n",
    "    print(f\"è¿‡æ»¤åæ•°æ®é‡: {filtered_count} æ¡\")\n",
    "    print(f\"ç§»é™¤çš„æœªåŒ¹é…æ•°æ®: {total_count - filtered_count} æ¡\")\n",
    "    print(\"=== è¿‡æ»¤æœªåŒ¹é…æ•°æ®å®Œæˆ ===\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def main():\n",
    "    # 1. è¯»å–å•†å“æ•°æ®ï¼ˆæ·»åŠ é”™è¯¯å¤„ç†ï¼‰\n",
    "    raw_data_path = r\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\matches_labubu.csv\"\n",
    "    try:\n",
    "        df_products = pd.read_csv(raw_data_path)\n",
    "        print(f\"âœ… æˆåŠŸåŠ è½½å•†å“æ•°æ®ï¼Œå…± {len(df_products)} æ¡\")\n",
    "        print(f\"æ•°æ®åŒ…å«åˆ—: {list(df_products.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–å•†å“æ•°æ®å¤±è´¥: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 2. æ•°æ®å»é‡\n",
    "    df_deduplicated = remove_duplicate_data(df_products)\n",
    "    \n",
    "    # 3. è¿‡æ»¤æœªåŒ¹é…æ•°æ®ï¼ˆæ–°å¢æ­¥éª¤ï¼šåˆ é™¤åŒ¹é…ç³»åˆ—ä¸ºã€ŒæœªåŒ¹é…ã€çš„æ•°æ®ï¼‰\n",
    "    df_final = filter_unmatched_data(df_deduplicated)\n",
    "    \n",
    "    # 4. ä¿å­˜æœ€ç»ˆç»“æœï¼ˆå»é‡+è¿‡æ»¤åçš„æœ‰æ•ˆæ•°æ®ï¼‰\n",
    "    final_save_path = r\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\matched_labubu.csv\"\n",
    "    try:\n",
    "        df_final.to_csv(final_save_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"\\nâœ… æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°: {final_save_path}\")\n",
    "        print(f\"âœ… æœ€ç»ˆæœ‰æ•ˆæ•°æ®é‡: {len(df_final)} æ¡ï¼ˆå»é‡+è¿‡æ»¤æœªåŒ¹é…åï¼‰\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}\")\n",
    "\n",
    "# æ‰§è¡Œä¸»å‡½æ•°\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4905b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimooæ¸…æ´—\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Dimooç³»åˆ—é…ç½®\n",
    "DIMOO_SERIES = [\n",
    "    \"Dimoo å¦‚æœä»Šå¤©æ˜ŸæœŸå…«ç³»åˆ—æ‰‹åŠ\",\n",
    "    \"Dimooå¿ƒåŠ¨ç‰¹è°ƒç³»åˆ—ï¼ˆè½¯è„¸æ¯›ç»’é’¥åŒ™æ‰£ç›²ç›’ï¼‰\",\n",
    "    \"Dimooä¾ç½—çºªä¸–ç•Œç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo è‡ªç„¶çš„å½¢çŠ¶ç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo WORLD Ã— è¿ªå£«å°¼ç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo æ¢¦é‡Œæ¢¦å¤–ç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo äº‘é™ªä¼´ç³»åˆ—ï¼ˆæ£‰èŠ±å¨ƒå¨ƒï¼‰\",\n",
    "    \"Dimoo åœ¨ä½ èº«è¾¹ç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo åŠ¨ç‰©ç‹å›½ç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo ä»Šæ™šä¸å‡†ç¡ç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo ç»å…¸å¤åˆ»ç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo æ—¶å…‰æ¼«æ¸¸ç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo æ°´æ—é¦†ç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo å»å“ªå„¿ç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo å®…å®…ç³»åˆ—ç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo æ£®æ—ä¹‹å¤œç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\",\n",
    "    \"Dimoo æ˜Ÿåº§ç³»åˆ—ï¼ˆå…¬ä»”ï¼‰\"\n",
    "]\n",
    "\n",
    "def create_dimoo_official_data():\n",
    "    \"\"\"\n",
    "    åˆ›å»ºDimooå®˜æ–¹ç³»åˆ—æ•°æ®\n",
    "    \"\"\"\n",
    "    official_products = []\n",
    "    \n",
    "    for series_name in DIMOO_SERIES:\n",
    "        # ä¸ºæ¯ä¸ªç³»åˆ—åˆ›å»ºå‡ ä¸ªä»£è¡¨æ€§çš„å•†å“\n",
    "        base_name = series_name.split('ï¼ˆ')[0].split('ç³»åˆ—')[0].strip()\n",
    "        \n",
    "        # æ ¹æ®ç³»åˆ—åç§°åˆ›å»ºä¸åŒçš„å•†å“å˜ä½“\n",
    "        if 'æ˜ŸæœŸå…«' in series_name:\n",
    "            products = [f\"{base_name} çº½æ‰£å¼€èŠ±\", f\"{base_name} æƒ³è¦æ™´å¤©\", f\"{base_name} åšé›•å¡‘\",f\"{base_name} è¢œå­å°ç‹—\",f\"{base_name} å’Œé‡‘é±¼é€šç”µè¯\",f\"{base_name} çœ‹æ¼«ç”»\",f\"{base_name} å¿«ä¹æ´—æ¾¡\",f\"{base_name} æºœæ°”çƒé¸­\",f\"{base_name} ç¡å¤§è§‰\",f\"{base_name} æ€è€ƒäººç”Ÿ\",f\"{base_name} äº‘å±‚ä¸Šè·³èˆ\",f\"{base_name} è§£é”æ—¶é—´\",f\"{base_name} æ—¥å†ä¸å¬è¯\"]\n",
    "        elif 'å¿ƒåŠ¨ç‰¹è°ƒ' in series_name:\n",
    "            products = [f\"{base_name} ç”Ÿæ¤°æ‹¿é“\", f\"{base_name} æµ·ç›åšä¹³\", f\"{base_name} ç„¦ç³–å¥¶å’–\",f\"{base_name} çº¢èŒ¶å†·èƒ\",f\"{base_name} è¥¿æŸšç¾å¼\",f\"{base_name} é¦¥éƒæ‘©å¡\",f\"{base_name} å¿ƒæ„æµ“ç¼©\"]\n",
    "        elif 'ä¾ç½—çºª' in series_name:\n",
    "            products = [f\"{base_name} é¾™è›‹\", f\"{base_name} å¹¼é¾™\", f\"{base_name} æˆé•¿è®°å½•å‘˜\",f\"{base_name} çƒå½¢æ¸¸è§ˆè½¦\",f\"{base_name} è…•é¾™\",f\"{base_name} ä¹å›­è§‚å…‰è½¦\",f\"{base_name} æé¾™\",f\"{base_name} ç¿¼é¾™\",f\"{base_name} æé¾™\"]\n",
    "        elif 'è‡ªç„¶çš„å½¢çŠ¶' in series_name:\n",
    "            products = [f\"{base_name} çœ‹è§\", f\"{base_name} æ°´æ»´\", f\"{base_name} æ ‘å¶\"]\n",
    "        elif 'è¿ªå£«å°¼' in series_name:\n",
    "            products = [f\"{base_name} ç±³å¥‡\", f\"{base_name} å”è€é¸­\", f\"{base_name} å°ç†Šç»´å°¼\"]\n",
    "        elif 'æ¢¦é‡Œæ¢¦å¤–' in series_name:\n",
    "            products = [f\"{base_name} ç¾æ¢¦\", f\"{base_name} æ¢¦å¢ƒ\", f\"{base_name} æ¢¦æ¸¸\"]\n",
    "        elif 'äº‘é™ªä¼´' in series_name:\n",
    "            products = [f\"{base_name} æ™´å¤©\", f\"{base_name} é›¨å¤©\", f\"{base_name} å½©è™¹\"]\n",
    "        elif 'åœ¨ä½ èº«è¾¹' in series_name:\n",
    "            products = [f\"{base_name} é™ªä¼´\", f\"{base_name} å®ˆæŠ¤\", f\"{base_name} ä¾é \"]\n",
    "        elif 'åŠ¨ç‰©ç‹å›½' in series_name:\n",
    "            products = [f\"{base_name} ç‹®å­\", f\"{base_name} å¤§è±¡\", f\"{base_name} é•¿é¢ˆé¹¿\"]\n",
    "        elif 'ä»Šæ™šä¸å‡†ç¡' in series_name:\n",
    "            products = [f\"{base_name} ç†¬å¤œ\", f\"{base_name} å¤±çœ \", f\"{base_name} å¤œçŒ«å­\"]\n",
    "        elif 'ç»å…¸å¤åˆ»' in series_name:\n",
    "            products = [f\"{base_name} å¤å¤\", f\"{base_name} æ€€æ—§\", f\"{base_name} ç»å…¸\"]\n",
    "        elif 'æ—¶å…‰æ¼«æ¸¸' in series_name:\n",
    "            products = [f\"{base_name} è¿‡å»\", f\"{base_name} ç°åœ¨\", f\"{base_name} æœªæ¥\"]\n",
    "        elif 'æ°´æ—é¦†' in series_name:\n",
    "            products = [f\"{base_name} é²¸é±¼\", f\"{base_name} æµ·è±š\", f\"{base_name} æ°´æ¯\"]\n",
    "        elif 'å»å“ªå„¿' in series_name:\n",
    "            products = [f\"{base_name} æ—…è¡Œ\", f\"{base_name} æ¢é™©\", f\"{base_name} æ¼«æ¸¸\"]\n",
    "        elif 'å®…å®…ç³»åˆ—' in series_name:\n",
    "            products = [f\"{base_name} å®…å®¶\", f\"{base_name} æ¸¸æˆ\", f\"{base_name} æ”¾æ¾\"]\n",
    "        elif 'æ£®æ—ä¹‹å¤œ' in series_name:\n",
    "            products = [f\"{base_name} çŒ«å¤´é¹°\", f\"{base_name} è¤ç«è™«\", f\"{base_name} æœˆå…‰\"]\n",
    "        elif 'æ˜Ÿåº§' in series_name:\n",
    "            products = [f\"{base_name} ç™½ç¾Šåº§\", f\"{base_name} é‡‘ç‰›åº§\", f\"{base_name} åŒå­åº§\"]\n",
    "        else:\n",
    "            products = [f\"{base_name} æ ‡å‡†æ¬¾\", f\"{base_name} ç‰¹åˆ«æ¬¾\", f\"{base_name} éšè—æ¬¾\"]\n",
    "        \n",
    "        for product_name in products:\n",
    "            official_products.append({\n",
    "                'å®˜æ–¹åç§°': product_name,\n",
    "                'ç³»åˆ—åç§°': series_name,\n",
    "                'å›¾ç‰‡URL': '',\n",
    "                'ç³»åˆ—URL': ''\n",
    "            })\n",
    "    \n",
    "    return official_products\n",
    "\n",
    "def convert_traditional_to_simplified(text):\n",
    "    \"\"\"\n",
    "    å°†ç¹ä½“ä¸­æ–‡è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "        \n",
    "    trad_to_simp = {\n",
    "        'æ˜ŸæœŸå…«': 'æ˜ŸæœŸå…«', 'å¿ƒå‹•': 'å¿ƒåŠ¨', 'ç‰¹èª¿': 'ç‰¹è°ƒ', 'ä¾ç¾…ç´€': 'ä¾ç½—çºª',\n",
    "        'ä¸–ç•Œ': 'ä¸–ç•Œ', 'è‡ªç„¶': 'è‡ªç„¶', 'å½¢ç‹€': 'å½¢çŠ¶', 'å¤¢è£¡': 'æ¢¦é‡Œ', 'å¤¢å¤–': 'æ¢¦å¤–',\n",
    "        'é›²é™ªä¼´': 'äº‘é™ªä¼´', 'æ£‰èŠ±': 'æ£‰èŠ±', 'å¨ƒå¨ƒ': 'å¨ƒå¨ƒ', 'å‹•ç‰©': 'åŠ¨ç‰©', 'ç‹åœ‹': 'ç‹å›½',\n",
    "        'ç¶“å…¸': 'ç»å…¸', 'å¾©åˆ»': 'å¤åˆ»', 'æ™‚å…‰': 'æ—¶å…‰', 'æ¼«éŠ': 'æ¼«æ¸¸', 'æ°´æ—é¤¨': 'æ°´æ—é¦†',\n",
    "        'å»å“ªå…’': 'å»å“ªå„¿', 'å®…å®…': 'å®…å®…', 'æ£®æ—': 'æ£®æ—', 'ä¹‹å¤œ': 'ä¹‹å¤œ', 'æ˜Ÿåº§': 'æ˜Ÿåº§',\n",
    "        'é‘°åŒ™': 'é’¥åŒ™', 'æ‰£ç›²': 'æ‰£ç›²', 'ç›’': 'ç›’', 'å…¬ä»”': 'å…¬ä»”', 'æ‰‹è¾¦': 'æ‰‹åŠ',\n",
    "        'è»Ÿè‡‰': 'è½¯è„¸', 'æ¯›çµ¨': 'æ¯›ç»’', 'è¿ªå£«å°¼': 'è¿ªå£«å°¼', 'é™ªä¼´': 'é™ªä¼´', 'ä¸å‡†': 'ä¸å‡†',\n",
    "        'ç¡è¦º': 'ç¡è§‰', 'å¾©å¤': 'å¤å¤', 'æ‡·èˆŠ': 'æ€€æ—§', 'æ—…è¡Œ': 'æ—…è¡Œ', 'æ¢éšª': 'æ¢é™©',\n",
    "        'éŠæˆ²': 'æ¸¸æˆ', 'æ”¾é¬†': 'æ”¾æ¾', 'è²“é ­é·¹': 'çŒ«å¤´é¹°', 'è¢ç«èŸ²': 'è¤ç«è™«', 'æœˆå…‰': 'æœˆå…‰'\n",
    "    }\n",
    "    \n",
    "    text_str = str(text)\n",
    "    for trad, simp in trad_to_simp.items():\n",
    "        text_str = text_str.replace(trad, simp)\n",
    "    return text_str\n",
    "\n",
    "def enhanced_dimoo_matching(df_products, df_official):\n",
    "    \"\"\"\n",
    "    å¢å¼ºçš„æ–‡æœ¬åŒ¹é…ç®—æ³•ï¼Œä¸“é—¨é’ˆå¯¹Dimooå•†å“\n",
    "    \"\"\"\n",
    "    \n",
    "    def dimoo_text_preprocess(text):\n",
    "        \"\"\"ä¸“é—¨é’ˆå¯¹Dimooå•†å“çš„æ–‡æœ¬é¢„å¤„ç†\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # ç¹ç®€è½¬æ¢\n",
    "        text = convert_traditional_to_simplified(text)\n",
    "        \n",
    "        # ä¿ç•™å…³é”®ä¿¡æ¯ï¼šæ•°å­—ã€ä¸­æ–‡ã€è‹±æ–‡ã€ç©ºæ ¼ã€ç‚¹å·\n",
    "        text = re.sub(r'[^\\w\\s\\.\\-\\u4e00-\\u9fff]', '', text)\n",
    "        \n",
    "        # Dimooç‰¹å®šçš„åœç”¨è¯\n",
    "        dimoo_stop_words = {\n",
    "            'pop', 'mart', 'dimoo', 'ç³»åˆ—', 'æ¬¾', 'ç‰ˆ', 'æ­£ç‰ˆ', 'å…¨æ–°', 'æœªæ‹†',\n",
    "            'ç°è´§', 'åŒ…é‚®', 'ç‰¹ä»·', 'ä¼˜æƒ ', 'å‡ºå”®', 'è½¬è®©', 'åŒ…æ­£', 'ä¿è¯', 'æ­£å“',\n",
    "            'æ‰‹åŠ', 'ç›²ç›’', 'æ¨¡å‹', 'ç©å…·', 'å…¬ä»”', 'åŸä»·', 'å‡º', 'æ¢', 'å–', 'ä¼˜æƒ '\n",
    "        }\n",
    "        \n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in dimoo_stop_words and len(word) > 0]\n",
    "        \n",
    "        if not filtered_words:\n",
    "            simple_text = re.sub(r'[^\\w\\s\\u4e00-\\u9fff]', '', str(text))\n",
    "            return simple_text.strip()\n",
    "        \n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    print(\"æ­£åœ¨é¢„å¤„ç†Dimooæ–‡æœ¬...\")\n",
    "    df_products['æ¸…æ´—åç§°'] = df_products['å•†å“åç§°'].apply(dimoo_text_preprocess)\n",
    "    df_official['æ¸…æ´—å®˜æ–¹åç§°'] = df_official['å®˜æ–¹åç§°'].apply(dimoo_text_preprocess)\n",
    "    \n",
    "    # æ·»åŠ è°ƒè¯•ä¿¡æ¯\n",
    "    print(\"\\n=== é¢„å¤„ç†ç¤ºä¾‹ ===\")\n",
    "    for i, (orig, cleaned) in enumerate(zip(df_products['å•†å“åç§°'].head(5), df_products['æ¸…æ´—åç§°'].head(5))):\n",
    "        print(f\"åŸå§‹: {orig} -> æ¸…æ´—: {cleaned}\")\n",
    "    \n",
    "    # ä½¿ç”¨å­—ç¬¦çº§åˆ«çš„TF-IDF\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer='char_wb',\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=1,\n",
    "        max_features=10000,\n",
    "        lowercase=False\n",
    "    )\n",
    "    \n",
    "    # åˆå¹¶æ‰€æœ‰æ–‡æœ¬è¿›è¡Œè®­ç»ƒ\n",
    "    all_texts = list(df_products['æ¸…æ´—åç§°']) + list(df_official['æ¸…æ´—å®˜æ–¹åç§°'])\n",
    "    vectorizer.fit(all_texts)\n",
    "    \n",
    "    products_tfidf = vectorizer.transform(df_products['æ¸…æ´—åç§°'])\n",
    "    official_tfidf = vectorizer.transform(df_official['æ¸…æ´—å®˜æ–¹åç§°'])\n",
    "    \n",
    "    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ\n",
    "    similarity_matrix = cosine_similarity(products_tfidf, official_tfidf)\n",
    "    \n",
    "    # æ‰¾åˆ°æœ€ä½³åŒ¹é…\n",
    "    matches = []\n",
    "    \n",
    "    print(f\"\\n=== åŒ¹é…å‰æ•°æ®æ£€æŸ¥ ===\")\n",
    "    print(f\"å¾…åŒ¹é…å•†å“æ•°é‡: {len(df_products)}\")\n",
    "    print(f\"å®˜æ–¹å•†å“æ•°é‡: {len(df_official)}\")\n",
    "    \n",
    "    for i, product_row in df_products.iterrows():\n",
    "        product_name = product_row['å•†å“åç§°']\n",
    "        cleaned_name = product_row['æ¸…æ´—åç§°']\n",
    "        \n",
    "        # å…³é”®è¯ä¼˜å…ˆåŒ¹é… - é’ˆå¯¹Dimooç³»åˆ—\n",
    "        matched_by_keyword = False\n",
    "        for series_info in DIMOO_SERIES:\n",
    "            series_simple = convert_traditional_to_simplified(series_info).lower()\n",
    "            # æå–ç³»åˆ—å…³é”®è¯\n",
    "            keywords = re.findall(r'[\\u4e00-\\u9fff]+', series_simple)\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç³»åˆ—å…³é”®è¯\n",
    "            if any(keyword in cleaned_name for keyword in keywords if len(keyword) > 1):\n",
    "                # æ‰¾åˆ°å¯¹åº”çš„å®˜æ–¹è®°å½•\n",
    "                series_match = df_official[df_official['ç³»åˆ—åç§°'] == series_info]\n",
    "                if not series_match.empty:\n",
    "                    official_match = series_match.iloc[0]\n",
    "                    matches.append({\n",
    "                        'å•†å“åç§°': product_name,\n",
    "                        'åŒ¹é…å®˜æ–¹åç§°': official_match['å®˜æ–¹åç§°'],\n",
    "                        'åŒ¹é…ç³»åˆ—': series_info,\n",
    "                        'åŒ¹é…å›¾ç‰‡URL': official_match.get('å›¾ç‰‡URL', ''),\n",
    "                        'åŒ¹é…åº¦': 0.95,  # é«˜åŒ¹é…åº¦\n",
    "                        'æ˜¯å¦åŒ¹é…æˆåŠŸ': True,\n",
    "                        'åŒ¹é…æ–¹å¼': 'å…³é”®è¯åŒ¹é…'\n",
    "                    })\n",
    "                    print(f\"âœ… é€šè¿‡å…³é”®è¯åŒ¹é…: {product_name} -> {series_info}\")\n",
    "                    matched_by_keyword = True\n",
    "                    break\n",
    "        \n",
    "        if matched_by_keyword:\n",
    "            continue\n",
    "        \n",
    "        # TF-IDFåŒ¹é…\n",
    "        best_match_idx = np.argmax(similarity_matrix[i])\n",
    "        best_score = similarity_matrix[i][best_match_idx]\n",
    "        \n",
    "        threshold = 0.15\n",
    "        \n",
    "        if best_score > threshold:\n",
    "            official_match = df_official.iloc[best_match_idx]\n",
    "            matches.append({\n",
    "                'å•†å“åç§°': product_name,\n",
    "                'åŒ¹é…å®˜æ–¹åç§°': official_match['å®˜æ–¹åç§°'],\n",
    "                'åŒ¹é…ç³»åˆ—': official_match['ç³»åˆ—åç§°'],\n",
    "                'åŒ¹é…å›¾ç‰‡URL': official_match.get('å›¾ç‰‡URL', ''),\n",
    "                'åŒ¹é…åº¦': round(best_score, 3),\n",
    "                'æ˜¯å¦åŒ¹é…æˆåŠŸ': True,\n",
    "                'åŒ¹é…æ–¹å¼': 'TF-IDFåŒ¹é…'\n",
    "            })\n",
    "        else:\n",
    "            matches.append({\n",
    "                'å•†å“åç§°': product_name,\n",
    "                'åŒ¹é…å®˜æ–¹åç§°': 'æœªåŒ¹é…åˆ°å®˜æ–¹åç§°',\n",
    "                'åŒ¹é…ç³»åˆ—': 'æœªåˆ†ç±»',\n",
    "                'åŒ¹é…å›¾ç‰‡URL': '',\n",
    "                'åŒ¹é…åº¦': round(best_score, 3),\n",
    "                'æ˜¯å¦åŒ¹é…æˆåŠŸ': False,\n",
    "                'åŒ¹é…æ–¹å¼': 'æœªåŒ¹é…'\n",
    "            })\n",
    "    \n",
    "    # åˆ›å»ºåŒ¹é…ç»“æœçš„DataFrame\n",
    "    df_matches = pd.DataFrame(matches)\n",
    "    \n",
    "    # æ£€æŸ¥åŒ¹é…ç»“æœæ˜¯å¦æœ‰é‡å¤\n",
    "    duplicate_matches = df_matches[df_matches.duplicated('å•†å“åç§°', keep=False)]\n",
    "    if not duplicate_matches.empty:\n",
    "        print(f\"âš ï¸  å‘ç° {len(duplicate_matches)} ä¸ªé‡å¤åŒ¹é…è®°å½•ï¼Œè¿›è¡Œå»é‡...\")\n",
    "        df_matches = df_matches.drop_duplicates(subset=['å•†å“åç§°'], keep='first')\n",
    "    \n",
    "    return df_matches\n",
    "\n",
    "def remove_duplicate_data(df, file_description=\"æ•°æ®\"):\n",
    "    \"\"\"\n",
    "    å‰”é™¤å•†å“åç§°ã€ä»·æ ¼ã€å‘å¸ƒç”¨æˆ·ã€å‘å¸ƒæ—¶é—´ã€å•†å“é“¾æ¥å’Œçˆ¬å–æ—¶é—´å®Œå…¨ä¸€æ ·çš„æ•°æ®\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== å¼€å§‹å¯¹ [{file_description}] è¿›è¡Œæ•°æ®å»é‡ ===\")\n",
    "    original_count = len(df)\n",
    "    print(f\"[{file_description}] å»é‡å‰æ•°æ®é‡: {original_count}\")\n",
    "    \n",
    "    # å®šä¹‰å»é‡çš„åˆ—\n",
    "    duplicate_columns = ['å•†å“åç§°', 'ä»·æ ¼', 'å‘å¸ƒç”¨æˆ·', 'å‘å¸ƒæ—¶é—´', 'å•†å“é“¾æ¥', 'çˆ¬å–æ—¶é—´']\n",
    "    \n",
    "    # æ£€æŸ¥å“ªäº›åˆ—å®é™…å­˜åœ¨äºDataFrameä¸­\n",
    "    available_columns = [col for col in duplicate_columns if col in df.columns]\n",
    "    print(f\"[{file_description}] ç”¨äºå»é‡çš„åˆ—: {available_columns}\")\n",
    "    \n",
    "    if not available_columns:\n",
    "        print(f\"âš ï¸  è­¦å‘Š: [{file_description}] ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æŒ‡å®šçš„å»é‡åˆ—ï¼Œè·³è¿‡å»é‡\")\n",
    "        return df\n",
    "    \n",
    "    # æ‰§è¡Œå»é‡ï¼ˆä¿ç•™ç¬¬ä¸€æ¡è®°å½•ï¼‰\n",
    "    df_deduplicated = df.drop_duplicates(subset=available_columns, keep='first')\n",
    "    final_count = len(df_deduplicated)\n",
    "    \n",
    "    removed_count = original_count - final_count\n",
    "    print(f\"[{file_description}] å»é‡åæ•°æ®é‡: {final_count}\")\n",
    "    print(f\"[{file_description}] ç§»é™¤çš„é‡å¤æ•°æ®: {removed_count} æ¡\")\n",
    "    \n",
    "    if original_count > 0:\n",
    "        deduplication_rate = (removed_count / original_count * 100)\n",
    "        print(f\"[{file_description}] å»é‡ç‡: {deduplication_rate:.2f}%\")\n",
    "    \n",
    "    return df_deduplicated\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Dimooæ•°æ®åŒ¹é…ä¸»ç¨‹åº\n",
    "    \"\"\"\n",
    "    # è¯»å–Dimooå•†å“æ•°æ®\n",
    "    try:\n",
    "        # è¯·æ ¹æ®æ‚¨çš„å®é™…æ–‡ä»¶è·¯å¾„ä¿®æ”¹\n",
    "        df_products = pd.read_csv(r\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\cleaned_dimoo.csv\")\n",
    "        print(f\"âœ… åŠ è½½äº† {len(df_products)} ä¸ªDimooå•†å“æ•°æ®\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–Dimooå•†å“æ•°æ®å¤±è´¥: {e}\")\n",
    "        print(\"è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®\")\n",
    "        return\n",
    "    \n",
    "    # æ•°æ®å»é‡\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ç¬¬ä¸€æ­¥ï¼šæ•°æ®å»é‡\")\n",
    "    print(\"=\"*50)\n",
    "    df_products = remove_duplicate_data(df_products, \"åŸå§‹Dimooæ•°æ®\")\n",
    "    \n",
    "    # ç¹ç®€è½¬æ¢\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ç¬¬äºŒæ­¥ï¼šç¹ç®€è½¬æ¢\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"æ­£åœ¨å°†å•†å“åç§°ç”±ç¹ä½“è½¬æ¢ä¸ºç®€ä½“...\")\n",
    "    df_products['å•†å“åç§°'] = df_products['å•†å“åç§°'].apply(convert_traditional_to_simplified)\n",
    "    print(\"âœ… å•†å“åç§°ç¹ç®€è½¬æ¢å®Œæˆ\")\n",
    "    \n",
    "    # æ˜¾ç¤ºè½¬æ¢åçš„ç¤ºä¾‹\n",
    "    print(\"\\n=== ç¹ç®€è½¬æ¢ç¤ºä¾‹ ===\")\n",
    "    for i, name in enumerate(df_products['å•†å“åç§°'].head(5)):\n",
    "        print(f\"ç¤ºä¾‹ {i+1}: {name}\")\n",
    "    \n",
    "    # åˆ›å»ºå®˜æ–¹æ•°æ®\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºå®˜æ–¹æ•°æ®\")\n",
    "    print(\"=\"*50)\n",
    "    official_products = create_dimoo_official_data()\n",
    "    df_official = pd.DataFrame(official_products)\n",
    "    print(f\"âœ… åˆ›å»ºäº† {len(df_official)} ä¸ªå®˜æ–¹Dimooå•†å“\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå®˜æ–¹ç³»åˆ—\n",
    "    print(\"\\n=== Dimooå®˜æ–¹ç³»åˆ— ===\")\n",
    "    for i, series in enumerate(DIMOO_SERIES, 1):\n",
    "        print(f\"{i:2d}. {series}\")\n",
    "    \n",
    "    # è¿›è¡ŒåŒ¹é…\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ç¬¬å››æ­¥ï¼šå•†å“åŒ¹é…\")\n",
    "    print(\"=\"*50)\n",
    "    df_matches = enhanced_dimoo_matching(df_products, df_official)\n",
    "    \n",
    "    # åˆå¹¶ç»“æœ\n",
    "    df_final = df_products.merge(df_matches, on='å•†å“åç§°', how='left')\n",
    "    \n",
    "    # æœ€ç»ˆæ•°æ®å»é‡\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ç¬¬äº”æ­¥ï¼šæœ€ç»ˆæ•°æ®å»é‡\")\n",
    "    print(\"=\"*50)\n",
    "    df_final = remove_duplicate_data(df_final, \"æœ€ç»ˆDimooåŒ¹é…ç»“æœ\")\n",
    "    \n",
    "    # åˆ†æåŒ¹é…ç»“æœ\n",
    "    match_stats = df_final['æ˜¯å¦åŒ¹é…æˆåŠŸ'].value_counts()\n",
    "    print(f\"\\n=== DimooåŒ¹é…ç»“æœç»Ÿè®¡ ===\")\n",
    "    print(f\"æˆåŠŸåŒ¹é…: {match_stats.get(True, 0)} ä»¶å•†å“\")\n",
    "    print(f\"æœªæˆåŠŸåŒ¹é…: {match_stats.get(False, 0)} ä»¶å•†å“\")\n",
    "    if len(df_final) > 0:\n",
    "        success_rate = match_stats.get(True, 0) / len(df_final) * 100\n",
    "        print(f\"åŒ¹é…æˆåŠŸç‡: {success_rate:.1f}%\")\n",
    "    \n",
    "    # æ˜¾ç¤ºåŒ¹é…æ–¹å¼ç»Ÿè®¡\n",
    "    if 'åŒ¹é…æ–¹å¼' in df_final.columns:\n",
    "        method_stats = df_final[df_final['æ˜¯å¦åŒ¹é…æˆåŠŸ'] == True]['åŒ¹é…æ–¹å¼'].value_counts()\n",
    "        print(f\"\\n=== åŒ¹é…æ–¹å¼ç»Ÿè®¡ ===\")\n",
    "        for method, count in method_stats.items():\n",
    "            print(f\"{method}: {count}ä»¶\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå„ç³»åˆ—ç»Ÿè®¡\n",
    "    if not df_final[df_final['æ˜¯å¦åŒ¹é…æˆåŠŸ'] == True].empty:\n",
    "        series_stats = df_final[df_final['æ˜¯å¦åŒ¹é…æˆåŠŸ'] == True]['åŒ¹é…ç³»åˆ—'].value_counts()\n",
    "        print(f\"\\n=== Dimooå„ç³»åˆ—å•†å“æ•°é‡ ===\")\n",
    "        for series, count in series_stats.items():\n",
    "            print(f\"{series}: {count}ä»¶\")\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    final_save_path = r\"D:\\AImportant\\ç•™å­¦\\æ¸¯åŸ\\course\\Sem A\\COM5507 Social Media Data\\Group Project\\Data\\matched_dimoo.csv\"\n",
    "    if safe_save_dataframe(df_final, final_save_path):\n",
    "        print(f\"\\nğŸ‰ DimooåŒ¹é…å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {final_save_path}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hazel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
