{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7972447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import hashlib\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def setup_driver_advanced():\n",
    "    \"\"\"é«˜çº§æµè§ˆå™¨è®¾ç½® - ç»•è¿‡æ£€æµ‹\"\"\"\n",
    "    chrome_options = Options()\n",
    "    \n",
    "    # åŸºç¡€è®¾ç½®\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    # é«˜çº§åæ£€æµ‹è®¾ç½®\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\", \"enable-logging\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    # ç¦ç”¨å›¾ç‰‡å’ŒCSSåŠ é€ŸåŠ è½½\n",
    "    chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-plugins\")\n",
    "    chrome_options.add_argument(\"--disable-javascript\")\n",
    "    \n",
    "    # éšæœºç”¨æˆ·ä»£ç†\n",
    "    user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    ]\n",
    "    selected_ua = random.choice(user_agents)\n",
    "    chrome_options.add_argument(f\"--user-agent={selected_ua}\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    # æ‰§è¡Œé«˜çº§åæ£€æµ‹è„šæœ¬\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})\")\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh', 'en']})\")\n",
    "    \n",
    "    # è®¾ç½®è¶…æ—¶\n",
    "    driver.set_page_load_timeout(30)\n",
    "    driver.set_script_timeout(30)\n",
    "    \n",
    "    return driver\n",
    "\n",
    "def scrape_carousell_advanced():\n",
    "    \"\"\"é«˜çº§çˆ¬è™« - çªç ´500æ¡é™åˆ¶\"\"\"\n",
    "    driver = None\n",
    "    all_products = []\n",
    "    seen_products = set()\n",
    "    \n",
    "    try:\n",
    "        driver = setup_driver_advanced()\n",
    "        \n",
    "        # ç­–ç•¥1: ä½¿ç”¨ä¸åŒçš„æœç´¢URLå’Œå‚æ•°\n",
    "        search_urls = [\n",
    "            \"https://www.carousell.com.hk/search/dimoo\",\n",
    "            \"https://www.carousell.com.hk/search/dimoo?sort_by=time_created%2Cdescending\",\n",
    "            \"https://www.carousell.com.hk/search/dimoo?sort_by=price%2Cascending\",\n",
    "            \"https://www.carousell.com.hk/search/dimoo?addRecent=false\"\n",
    "        ]\n",
    "        \n",
    "        for search_url in search_urls:\n",
    "            if len(all_products) >= 1000:  # å¦‚æœå·²ç»æ”¶é›†è¶³å¤Ÿå¤šæ•°æ®\n",
    "                break\n",
    "                \n",
    "            print(f\"\\nğŸ¯ å°è¯•URL: {search_url}\")\n",
    "            products_from_url = scrape_with_url(driver, search_url, seen_products)\n",
    "            all_products.extend(products_from_url)\n",
    "            print(f\"ä»è¯¥URLè·å¾— {len(products_from_url)} ä¸ªå•†å“\")\n",
    "            \n",
    "            # ä¼‘æ¯ä¸€ä¸‹é¿å…è¢«å°\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "        \n",
    "        print(f\"\\næ€»å…±æ”¶é›†åˆ° {len(all_products)} ä¸ªå”¯ä¸€å•†å“\")\n",
    "        \n",
    "        if all_products:\n",
    "            saved_file = save_advanced_data(all_products)\n",
    "            return all_products, saved_file\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"é«˜çº§çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        if all_products:\n",
    "            saved_file = save_advanced_data(all_products)\n",
    "            return all_products, saved_file\n",
    "        return None, None\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "def scrape_with_url(driver, search_url, seen_products):\n",
    "    \"\"\"ä½¿ç”¨ç‰¹å®šURLçˆ¬å–\"\"\"\n",
    "    products = []\n",
    "    \n",
    "    try:\n",
    "        print(f\"è®¿é—®: {search_url}\")\n",
    "        driver.get(search_url)\n",
    "        \n",
    "        # ç­‰å¾…é¡µé¢åŠ è½½\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # æ¨¡æ‹Ÿäººç±»è¡Œä¸ºæ»šåŠ¨\n",
    "        human_like_scroll(driver)\n",
    "        \n",
    "        max_pages = 20  # æ¯URLæœ€å¤š20é¡µ\n",
    "        current_page = 1\n",
    "        \n",
    "        while current_page <= max_pages:\n",
    "            print(f\"å¤„ç†ç¬¬ {current_page} é¡µ...\")\n",
    "            \n",
    "            # æ·±åº¦æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹\n",
    "            deep_scroll_for_content(driver)\n",
    "            \n",
    "            # æå–å½“å‰é¡µå•†å“\n",
    "            page_products = extract_products_advanced(driver, seen_products)\n",
    "            products.extend(page_products)\n",
    "            \n",
    "            print(f\"ç¬¬ {current_page} é¡µæ‰¾åˆ° {len(page_products)} ä¸ªæ–°å•†å“\")\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µé¢\n",
    "            if not has_more_content(driver):\n",
    "                print(\"æ²¡æœ‰æ›´å¤šå†…å®¹ï¼Œåœæ­¢ç¿»é¡µ\")\n",
    "                break\n",
    "                \n",
    "            # å°è¯•ç¿»é¡µ\n",
    "            if not smart_pagination(driver, current_page):\n",
    "                print(\"æ— æ³•ç¿»é¡µï¼Œåœæ­¢\")\n",
    "                break\n",
    "                \n",
    "            current_page += 1\n",
    "            \n",
    "            # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º\n",
    "            time.sleep(random.uniform(3, 8))\n",
    "            \n",
    "            # æ¯5é¡µä¼‘æ¯æ›´é•¿æ—¶é—´\n",
    "            if current_page % 5 == 0:\n",
    "                print(\"ä¼‘æ¯ä¸€ä¸‹...\")\n",
    "                time.sleep(random.uniform(10, 15))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"è¯¥URLçˆ¬å–å¤±è´¥: {e}\")\n",
    "    \n",
    "    return products\n",
    "\n",
    "def human_like_scroll(driver):\n",
    "    \"\"\"æ¨¡æ‹Ÿäººç±»æ»šåŠ¨è¡Œä¸º\"\"\"\n",
    "    print(\"æ¨¡æ‹Ÿäººç±»æ»šåŠ¨...\")\n",
    "    \n",
    "    scroll_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    viewport_height = driver.execute_script(\"return window.innerHeight\")\n",
    "    \n",
    "    # åˆ†æ®µæ»šåŠ¨ï¼Œå¸¦æœ‰éšæœºåœé¡¿\n",
    "    scroll_positions = [\n",
    "        viewport_height * 0.3,\n",
    "        viewport_height * 0.6,\n",
    "        viewport_height * 0.9,\n",
    "        scroll_height * 0.2,\n",
    "        scroll_height * 0.4,\n",
    "        scroll_height * 0.6,\n",
    "        scroll_height * 0.8,\n",
    "        scroll_height\n",
    "    ]\n",
    "    \n",
    "    for i, position in enumerate(scroll_positions):\n",
    "        # éšæœºæ»šåŠ¨é€Ÿåº¦\n",
    "        scroll_script = f\"\"\"\n",
    "        window.scrollTo({{\n",
    "            top: {position},\n",
    "            behavior: 'smooth'\n",
    "        }});\n",
    "        \"\"\"\n",
    "        driver.execute_script(scroll_script)\n",
    "        \n",
    "        # éšæœºç­‰å¾…æ—¶é—´\n",
    "        wait_time = random.uniform(0.5, 2.5)\n",
    "        time.sleep(wait_time)\n",
    "        \n",
    "        # å¶å°”éšæœºå°èŒƒå›´æ»šåŠ¨\n",
    "        if random.random() < 0.3:\n",
    "            small_scroll = random.randint(-100, 100)\n",
    "            driver.execute_script(f\"window.scrollBy(0, {small_scroll})\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "def deep_scroll_for_content(driver):\n",
    "    \"\"\"æ·±åº¦æ»šåŠ¨ä»¥åŠ è½½æ›´å¤šå†…å®¹\"\"\"\n",
    "    print(\"æ·±åº¦æ»šåŠ¨åŠ è½½å†…å®¹...\")\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    no_change_count = 0\n",
    "    \n",
    "    for i in range(10):  # æœ€å¤šå°è¯•10æ¬¡\n",
    "        # æ»šåŠ¨åˆ°åº•éƒ¨\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦æœ‰\"Load More\"æŒ‰é’®å¹¶ç‚¹å‡»\n",
    "        if click_load_more_buttons(driver):\n",
    "            time.sleep(3)\n",
    "        \n",
    "        # æ£€æŸ¥é«˜åº¦æ˜¯å¦å˜åŒ–\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            no_change_count += 1\n",
    "            if no_change_count >= 3:\n",
    "                print(\"è¿ç»­3æ¬¡æ»šåŠ¨é«˜åº¦æ— å˜åŒ–ï¼Œåœæ­¢æ»šåŠ¨\")\n",
    "                break\n",
    "        else:\n",
    "            no_change_count = 0\n",
    "            print(f\"å‘ç°æ–°å†…å®¹ï¼Œé¡µé¢é«˜åº¦: {new_height}\")\n",
    "        \n",
    "        last_height = new_height\n",
    "        \n",
    "        # éšæœºç­‰å¾…\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "def click_load_more_buttons(driver):\n",
    "    \"\"\"ç‚¹å‡»æ‰€æœ‰å¯èƒ½çš„åŠ è½½æ›´å¤šæŒ‰é’®\"\"\"\n",
    "    load_more_selectors = [\n",
    "        \"button[data-testid*='load-more']\",\n",
    "        \"button:contains('Load More')\",\n",
    "        \"button:contains('é¡¯ç¤ºæ›´å¤š')\",\n",
    "        \"button:contains('Show More')\",\n",
    "        \".load-more\",\n",
    "        \"[class*='load-more']\"\n",
    "    ]\n",
    "    \n",
    "    for selector in load_more_selectors:\n",
    "        try:\n",
    "            # ä½¿ç”¨XPathåŒ…å«æ–‡æœ¬çš„æŸ¥æ‰¾\n",
    "            buttons = driver.find_elements(By.XPATH, \n",
    "                f\"//button[contains(text(), 'Load More') or contains(text(), 'Show More') or contains(text(), 'é¡¯ç¤ºæ›´å¤š')]\")\n",
    "            \n",
    "            for button in buttons:\n",
    "                try:\n",
    "                    if button.is_displayed() and button.is_enabled():\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});\", button)\n",
    "                        time.sleep(1)\n",
    "                        driver.execute_script(\"arguments[0].click();\", button)\n",
    "                        print(\"âœ… ç‚¹å‡»äº†åŠ è½½æ›´å¤šæŒ‰é’®\")\n",
    "                        return True\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return False\n",
    "\n",
    "def extract_products_advanced(driver, seen_products):\n",
    "    \"\"\"é«˜çº§å•†å“æå–\"\"\"\n",
    "    products = []\n",
    "    \n",
    "    # å¤šç§å…ƒç´ å®šä½ç­–ç•¥\n",
    "    extraction_strategies = [\n",
    "        extract_by_data_testid,\n",
    "        extract_by_class_patterns,\n",
    "        extract_by_link_analysis,\n",
    "        extract_by_structural_analysis\n",
    "    ]\n",
    "    \n",
    "    all_found_products = []\n",
    "    \n",
    "    for strategy in extraction_strategies:\n",
    "        try:\n",
    "            found_products = strategy(driver, seen_products)\n",
    "            if found_products:\n",
    "                all_found_products.extend(found_products)\n",
    "                print(f\"ç­–ç•¥ {strategy.__name__} æ‰¾åˆ° {len(found_products)} ä¸ªå•†å“\")\n",
    "                \n",
    "                # å¦‚æœæ‰¾åˆ°å¾ˆå¤šå•†å“ï¼Œå¯ä»¥æå‰åœæ­¢\n",
    "                if len(all_found_products) > 50:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"ç­–ç•¥ {strategy.__name__} å¤±è´¥: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # å»é‡å¹¶æ·»åŠ åˆ°ç»“æœ\n",
    "    for product in all_found_products:\n",
    "        product_id = create_product_id(product)\n",
    "        if product_id not in seen_products:\n",
    "            seen_products.add(product_id)\n",
    "            products.append(product)\n",
    "    \n",
    "    return products\n",
    "\n",
    "def extract_by_data_testid(driver, seen_products):\n",
    "    \"\"\"é€šè¿‡data-testidå±æ€§æå–\"\"\"\n",
    "    products = []\n",
    "    \n",
    "    testid_patterns = [\n",
    "        'listing-card',\n",
    "        'product-card',\n",
    "        'listing',\n",
    "        'product',\n",
    "        'item'\n",
    "    ]\n",
    "    \n",
    "    for pattern in testid_patterns:\n",
    "        selector = f'[data-testid*=\"{pattern}\"]'\n",
    "        try:\n",
    "            elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            for element in elements:\n",
    "                product = extract_from_element(element)\n",
    "                if product and product not in products:\n",
    "                    products.append(product)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return products\n",
    "\n",
    "def extract_by_class_patterns(driver, seen_products):\n",
    "    \"\"\"é€šè¿‡ç±»åæ¨¡å¼æå–\"\"\"\n",
    "    products = []\n",
    "    \n",
    "    class_patterns = [\n",
    "        'listingCard',\n",
    "        'productCard',\n",
    "        'listing',\n",
    "        'card',\n",
    "        'item'\n",
    "    ]\n",
    "    \n",
    "    for pattern in class_patterns:\n",
    "        selectors = [\n",
    "            f'div[class*=\"{pattern}\"]',\n",
    "            f'article[class*=\"{pattern}\"]',\n",
    "            f'section[class*=\"{pattern}\"]',\n",
    "            f'a[class*=\"{pattern}\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                for element in elements:\n",
    "                    product = extract_from_element(element)\n",
    "                    if product and product not in products:\n",
    "                        products.append(product)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return products\n",
    "\n",
    "def extract_by_link_analysis(driver, seen_products):\n",
    "    \"\"\"é€šè¿‡é“¾æ¥åˆ†ææå–\"\"\"\n",
    "    products = []\n",
    "    \n",
    "    try:\n",
    "        # æŸ¥æ‰¾æ‰€æœ‰å•†å“é“¾æ¥\n",
    "        link_elements = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/p/\"]')\n",
    "        \n",
    "        for link_element in link_elements:\n",
    "            try:\n",
    "                # è·å–é“¾æ¥å‘¨å›´çš„å®¹å™¨\n",
    "                container = link_element\n",
    "                for i in range(3):\n",
    "                    try:\n",
    "                        container = container.find_element(By.XPATH, \"./..\")\n",
    "                        if container.text and len(container.text.strip()) > 30:\n",
    "                            product = extract_from_element(container)\n",
    "                            if product and product not in products:\n",
    "                                products.append(product)\n",
    "                            break\n",
    "                    except:\n",
    "                        break\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"é“¾æ¥åˆ†æå¤±è´¥: {e}\")\n",
    "    \n",
    "    return products\n",
    "\n",
    "def extract_by_structural_analysis(driver, seen_products):\n",
    "    \"\"\"é€šè¿‡ç»“æ„åˆ†ææå–\"\"\"\n",
    "    products = []\n",
    "    \n",
    "    try:\n",
    "        # æŸ¥æ‰¾åŒ…å«å•†å“ä¿¡æ¯çš„ç»“æ„å—\n",
    "        structural_selectors = [\n",
    "            'div > div > div',  # ä¸‰å±‚divç»“æ„\n",
    "            'article > div',\n",
    "            'section > div',\n",
    "            '[role=\"listitem\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in structural_selectors:\n",
    "            try:\n",
    "                elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                for element in elements:\n",
    "                    text = element.text.strip()\n",
    "                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å•†å“ç‰¹å¾\n",
    "                    if (len(text) > 50 and \n",
    "                        ('dimoo' in text.lower() or 'HK$' in text or '$' in text)):\n",
    "                        product = extract_from_element(element)\n",
    "                        if product and product not in products:\n",
    "                            products.append(product)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"ç»“æ„åˆ†æå¤±è´¥: {e}\")\n",
    "    \n",
    "    return products\n",
    "\n",
    "def extract_from_element(element):\n",
    "    \"\"\"ä»å…ƒç´ æå–å•†å“ä¿¡æ¯\"\"\"\n",
    "    try:\n",
    "        text = element.text.strip()\n",
    "        if len(text) < 30:\n",
    "            return None\n",
    "        \n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        \n",
    "        if len(lines) < 3:\n",
    "            return None\n",
    "        \n",
    "        # æå–ä¿¡æ¯\n",
    "        title = find_title(lines)\n",
    "        price = find_price(lines)\n",
    "        seller = find_seller(lines)\n",
    "        time_text = find_time(lines)\n",
    "        link = find_link(element)\n",
    "        \n",
    "        if not title:\n",
    "            return None\n",
    "            \n",
    "        return {\n",
    "            'title': clean_text(title),\n",
    "            'price': price or 'åƒ¹æ ¼å¾…è©¢',\n",
    "            'seller': clean_text(seller) if seller else '',\n",
    "            'post_time': clean_text(time_text) if time_text else '',\n",
    "            'link': link,\n",
    "            'crawl_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def find_title(lines):\n",
    "    \"\"\"æŸ¥æ‰¾æ ‡é¢˜\"\"\"\n",
    "    # ä¼˜å…ˆæŸ¥æ‰¾åŒ…å«dimooçš„è¡Œ\n",
    "    for line in lines:\n",
    "        if 'dimoo' in line.lower() and 10 < len(line) < 100:\n",
    "            return line\n",
    "    \n",
    "    # è¿”å›æœ€é•¿çš„åˆé€‚è¡Œ\n",
    "    candidate_lines = [line for line in lines if 20 < len(line) < 150]\n",
    "    if candidate_lines:\n",
    "        return max(candidate_lines, key=len)\n",
    "    \n",
    "    return lines[0] if lines else \"\"\n",
    "\n",
    "def find_price(lines):\n",
    "    \"\"\"æŸ¥æ‰¾ä»·æ ¼\"\"\"\n",
    "    for line in lines:\n",
    "        price_match = re.search(r'(HK\\$|\\$)\\s*(\\d+(?:,\\d+)*(?:\\.\\d+)?)', line)\n",
    "        if price_match:\n",
    "            return f\"{price_match.group(1)}{price_match.group(2)}\"\n",
    "    \n",
    "    for line in lines:\n",
    "        if 'free' in line.lower() or 'å…è²»' in line:\n",
    "            return \"FREE\"\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def find_seller(lines):\n",
    "    \"\"\"æŸ¥æ‰¾å–å®¶\"\"\"\n",
    "    if len(lines) > 0:\n",
    "        return lines[0]\n",
    "    return \"\"\n",
    "\n",
    "def find_time(lines):\n",
    "    \"\"\"æŸ¥æ‰¾æ—¶é—´\"\"\"\n",
    "    time_patterns = [\n",
    "        r'\\d+\\s*(åˆ†é˜|åˆ†é’Ÿ|å°æ™‚|å°æ—¶|å¤©|æ—¥|æœˆ|å¹´)\\s*å‰',\n",
    "        r'\\d+\\s*(minute|hour|day|week|month|year)s?\\s*ago',\n",
    "        r'å‰›å‰›|å‰›æ‰|just now'\n",
    "    ]\n",
    "    \n",
    "    for line in lines:\n",
    "        for pattern in time_patterns:\n",
    "            if re.search(pattern, line, re.IGNORECASE):\n",
    "                return line\n",
    "    return \"\"\n",
    "\n",
    "def find_link(element):\n",
    "    \"\"\"æŸ¥æ‰¾é“¾æ¥\"\"\"\n",
    "    try:\n",
    "        if element.tag_name == \"a\":\n",
    "            return element.get_attribute(\"href\")\n",
    "        else:\n",
    "            link_elem = element.find_element(By.CSS_SELECTOR, 'a[href*=\"/p/\"]')\n",
    "            return link_elem.get_attribute(\"href\") if link_elem else \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"æ¸…ç†æ–‡æœ¬\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def has_more_content(driver):\n",
    "    \"\"\"æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šå†…å®¹\"\"\"\n",
    "    try:\n",
    "        # æ£€æŸ¥åŠ è½½æ›´å¤šæŒ‰é’®\n",
    "        load_more_buttons = driver.find_elements(By.XPATH,\n",
    "            \"//button[contains(text(), 'Load More') or contains(text(), 'Show More') or contains(text(), 'é¡¯ç¤ºæ›´å¤š')]\"\n",
    "        )\n",
    "        if load_more_buttons:\n",
    "            return True\n",
    "        \n",
    "        # æ£€æŸ¥ä¸‹ä¸€é¡µæŒ‰é’®\n",
    "        next_buttons = driver.find_elements(By.XPATH,\n",
    "            \"//button[contains(text(), 'Next') or contains(text(), 'ä¸‹ä¸€é ')]\"\n",
    "        )\n",
    "        if next_buttons:\n",
    "            return True\n",
    "            \n",
    "        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾åº•éƒ¨\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        client_height = driver.execute_script(\"return document.documentElement.clientHeight\")\n",
    "        scroll_top = driver.execute_script(\"return document.documentElement.scrollTop\")\n",
    "        \n",
    "        if scroll_top + client_height < scroll_height - 100:\n",
    "            return True\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "def smart_pagination(driver, current_page):\n",
    "    \"\"\"æ™ºèƒ½ç¿»é¡µ\"\"\"\n",
    "    try:\n",
    "        # æ–¹æ³•1: ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®\n",
    "        next_buttons = driver.find_elements(By.XPATH,\n",
    "            \"//button[contains(text(), 'Next') or contains(text(), 'ä¸‹ä¸€é ')]\"\n",
    "        )\n",
    "        \n",
    "        for button in next_buttons:\n",
    "            try:\n",
    "                if button.is_displayed() and button.is_enabled():\n",
    "                    driver.execute_script(\"arguments[0].click();\", button)\n",
    "                    print(\"âœ… ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®\")\n",
    "                    time.sleep(3)\n",
    "                    return True\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # æ–¹æ³•2: URLç¿»é¡µ\n",
    "        next_url = f\"https://www.carousell.com.hk/search/dimoo?page={current_page + 1}\"\n",
    "        try:\n",
    "            driver.get(next_url)\n",
    "            print(f\"âœ… è·³è½¬åˆ°ç¬¬ {current_page + 1} é¡µ\")\n",
    "            time.sleep(3)\n",
    "            return True\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # æ–¹æ³•3: æ»šåŠ¨åŠ è½½\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ç¿»é¡µå¤±è´¥: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_product_id(product_info):\n",
    "    \"\"\"åˆ›å»ºå•†å“å”¯ä¸€æ ‡è¯†\"\"\"\n",
    "    key_string = f\"{product_info['title']}_{product_info['price']}_{product_info.get('link', '')}\"\n",
    "    return hashlib.md5(key_string.encode('utf-8')).hexdigest()\n",
    "\n",
    "def save_advanced_data(products):\n",
    "    \"\"\"ä¿å­˜é«˜çº§æ•°æ®\"\"\"\n",
    "    if not products:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "        filename = f\"carousell_dimoo_advanced_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "        filepath = os.path.join(desktop_path, filename)\n",
    "        \n",
    "        df = pd.DataFrame(products)\n",
    "        \n",
    "        # åˆ—æ˜ å°„\n",
    "        column_mapping = {\n",
    "            'title': 'å•†å“åç§°',\n",
    "            'price': 'ä»·æ ¼',\n",
    "            'seller': 'å‘å¸ƒç”¨æˆ·', \n",
    "            'post_time': 'å‘å¸ƒæ—¶é—´',\n",
    "            'link': 'å•†å“é“¾æ¥',\n",
    "            'crawl_time': 'çˆ¬å–æ—¶é—´'\n",
    "        }\n",
    "        \n",
    "        df = df.rename(columns=column_mapping)\n",
    "        available_columns = [col for col in column_mapping.values() if col in df.columns]\n",
    "        df = df[available_columns]\n",
    "        \n",
    "        df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "        print(f\"âœ… é«˜çº§æ•°æ®å·²ä¿å­˜: {filepath}\")\n",
    "        print(f\"ğŸ“Š æ€»æ•°æ®é‡: {len(df)} æ¡è®°å½•\")\n",
    "        \n",
    "        return filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ä¿å­˜å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_mass_scraper():\n",
    "    \"\"\"è¿è¡Œå¤§è§„æ¨¡çˆ¬å–\"\"\"\n",
    "    print(\"ğŸš€ å¼€å§‹å¤§è§„æ¨¡çˆ¬å–Carousell Mollyå•†å“...\")\n",
    "    print(\"ğŸ¯ ç›®æ ‡: çªç ´500æ¡é™åˆ¶ï¼Œè·å–1000+æ¡æ•°æ®\")\n",
    "    \n",
    "    all_products = []\n",
    "    max_attempts = 3\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"å°è¯• {attempt + 1}/{max_attempts}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        products, saved_file = scrape_carousell_advanced()\n",
    "        \n",
    "        if products:\n",
    "            all_products.extend(products)\n",
    "            print(f\"âœ… æœ¬æ¬¡è·å– {len(products)} æ¡æ•°æ®\")\n",
    "            print(f\"ğŸ“Š ç´¯è®¡è·å– {len(all_products)} æ¡æ•°æ®\")\n",
    "            \n",
    "            if len(all_products) >= 800:\n",
    "                print(\"ğŸ‰ å·²è¾¾åˆ°ç›®æ ‡æ•°æ®é‡!\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"âŒ æœ¬æ¬¡çˆ¬å–å¤±è´¥\")\n",
    "        \n",
    "        # é•¿æ—¶é—´ä¼‘æ¯é¿å…è¢«å°\n",
    "        if attempt < max_attempts - 1:\n",
    "            wait_time = random.randint(30, 60)\n",
    "            print(f\"â° ç­‰å¾… {wait_time} ç§’åç»§ç»­...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    # æœ€ç»ˆä¿å­˜\n",
    "    if all_products:\n",
    "        final_file = save_advanced_data(all_products)\n",
    "        print(f\"\\nğŸ‰ æœ€ç»ˆç»“æœ: æˆåŠŸè·å– {len(all_products)} æ¡æ•°æ®!\")\n",
    "        print(f\"ğŸ’¾ ä¿å­˜åˆ°: {final_file}\")\n",
    "        return all_products, final_file\n",
    "    else:\n",
    "        print(\"âŒ æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # è¿è¡Œå¤§è§„æ¨¡çˆ¬å–\n",
    "    products, saved_file = run_mass_scraper()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
